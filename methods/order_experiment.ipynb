{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MfIHYuHuTAp",
        "outputId": "7c41aa23-ad2f-4833-ceec-157fb09618dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myvhYB1ejlwt",
        "outputId": "0fc760ed-01f8-4103-a87a-fbd2442455dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.78.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "import asyncio\n",
        "import json, os, re, tempfile, time, math\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any\n",
        "from openai import OpenAI\n",
        "from copy import deepcopy\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2v3U-txuRyy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] =\n",
        "os.environ[\"CACHE_DIR\"] = \"./model_cache\"\n",
        "\n",
        "\n",
        "print(\"OPENAI_API_KEY:\", os.environ.get(\"OPENAI_API_KEY\"))\n",
        "print(\"CACHE_DIR:\", os.environ.get(\"CACHE_DIR\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im_9GeYmvZCk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import abc\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import math\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def chunkify(lst, n):\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i : i + n]\n",
        "\n",
        "# Base abstract class for model pipelines.\n",
        "class ModelPipelineBase(abc.ABC):\n",
        "    @abc.abstractmethod\n",
        "    def call_model(self, prompt: str) -> str:\n",
        "        \"\"\"Send a prompt to the model and return its response.\"\"\"\n",
        "        pass\n",
        "    def call_model_batch(self, prompts: list[str], batch_size: int = 16) -> list[str]:\n",
        "        \"\"\"\n",
        "        Split `prompts` into chunks of size batch_size, then call `call_model`\n",
        "        on each chunk in parallel threads.\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        for chunk in chunkify(prompts, batch_size):\n",
        "            with ThreadPoolExecutor(max_workers=len(chunk)) as ex:\n",
        "                futures = [ex.submit(self.call_model, p) for p in chunk]\n",
        "                for f in futures:\n",
        "                    results.append(f.result())\n",
        "        return results\n",
        "\n",
        "# Detailed Hugging Face pipeline that uses AutoTokenizer and AutoModelForCausalLM.\n",
        "class HuggingFaceDetailedPipeline(ModelPipelineBase):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str,\n",
        "        cache_dir: str = None,\n",
        "        torch_dtype: torch.dtype = torch.float16,\n",
        "        temperature: float = 0.0,\n",
        "        device: int = -1,  # -1 means CPU; set to GPU device index if available.\n",
        "        max_length: int = 150\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the Hugging Face pipeline.\n",
        "        :param model_name: Name or path of the model.\n",
        "        :param cache_dir: Directory to cache the model. If not provided in argument or environment, no cache_dir is used.\n",
        "        :param torch_dtype: The torch dtype for the model weights.\n",
        "        :param temperature: Sampling temperature (0 for deterministic).\n",
        "        :param device: Device index (-1 for CPU, >=0 for GPU).\n",
        "        :param max_length: Maximum length of generated text.\n",
        "        \"\"\"\n",
        "        self.cache_dir = cache_dir or os.environ.get(\"CACHE_DIR\")\n",
        "        self.model_name = model_name\n",
        "        self.torch_dtype = torch_dtype\n",
        "        self.temperature = temperature\n",
        "        self.device = device\n",
        "        self.max_length = max_length\n",
        "\n",
        "        if self.cache_dir:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=self.cache_dir)\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                cache_dir=self.cache_dir,\n",
        "                torch_dtype=self.torch_dtype\n",
        "            )\n",
        "        else:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                torch_dtype=self.torch_dtype\n",
        "            )\n",
        "\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        if self.device >= 0:\n",
        "            self.model.to(torch.device(f\"cuda:{self.device}\"))\n",
        "        else:\n",
        "            self.model.to(torch.device(\"cpu\"))\n",
        "\n",
        "    def call_model(self, prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Generate a response from the model given the prompt.\n",
        "        \"\"\"\n",
        "        return self._generate(prompt)\n",
        "\n",
        "    def _generate(self, prompt: str) -> str:\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "        # Move inputs to the appropriate device.\n",
        "        inputs = {key: value.to(self.model.device) for key, value in inputs.items()}\n",
        "        outputs = self.model.generate(\n",
        "            **inputs,\n",
        "            max_length=self.max_length,\n",
        "            temperature=self.temperature,\n",
        "            pad_token_id=self.tokenizer.eos_token_id,\n",
        "            do_sample=self.temperature > 0  # If temperature==0, generation is deterministic.\n",
        "        )\n",
        "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return generated_text\n",
        "\n",
        "\n",
        "class GPTAPIPipeline(ModelPipelineBase):\n",
        "    def __init__(self, model: str = \"gpt-3.5-turbo\", temperature: float = 0.0, max_tokens: int = 150):\n",
        "        \"\"\"\n",
        "        Initialize the API pipeline.\n",
        "        Expects the OPENAI_API_KEY to be set in environment variables.\n",
        "        :param model: The model name to use (e.g., 'gpt-3.5-turbo').\n",
        "        :param temperature: Sampling temperature for the API (default 0.0 for deterministic output).\n",
        "        :param max_tokens: Maximum tokens to generate.\n",
        "        \"\"\"\n",
        "        self.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            raise ValueError(\"OPENAI_API_KEY is not set in the environment variables.\")\n",
        "        self.model = model\n",
        "        self.temperature = temperature\n",
        "        self.max_tokens = max_tokens\n",
        "        self.client = OpenAI(api_key=self.api_key)\n",
        "\n",
        "    def call_model(self, prompt: str) -> str:\n",
        "        return self._api_call(prompt)\n",
        "\n",
        "    def _api_call(self, prompt: str) -> str:\n",
        "\n",
        "        completion = self.client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=self.temperature,\n",
        "            max_tokens=self.max_tokens,\n",
        "        )\n",
        "        return completion.choices[0].message.content\n",
        "\n",
        "class OpenAIBatchPipeline(ModelPipelineBase):\n",
        "    \"\"\"\n",
        "    Re-implements `call_model_batch` using the Batch API.\n",
        "    `call_model` falls back to the sync endpoint so your existing single-call\n",
        "    code (e.g. debugging) still works.\n",
        "    \"\"\"\n",
        "    def __init__(self, max_tokens: int = 150, temperature: float = 0.0):\n",
        "        self.max_tokens  = max_tokens\n",
        "        self.temperature = temperature\n",
        "\n",
        "    # --- single prompt (rare) ------------------------------------------------\n",
        "    def call_model(self, prompt: str) -> str:\n",
        "        resp = client.chat.completions.create(\n",
        "            model       = OPENAI_MODEL_ID,\n",
        "            messages    = [{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens  = self.max_tokens,\n",
        "            temperature = self.temperature\n",
        "        )\n",
        "        return resp.choices[0].message.content\n",
        "\n",
        "    # --- many prompts (main path) -------------------------------------------\n",
        "    def call_model_batch(self, prompts: List[str],\n",
        "                         batch_size: int = 64) -> List[str]:\n",
        "        \"\"\"\n",
        "        `batch_size` is irrelevant now; we always send the whole list through\n",
        "        the Batch API, chunked automatically by `run_batch`.\n",
        "        \"\"\"\n",
        "        lines = [_make_jsonl_line(f\"req-{i}\", p, self.max_tokens,\n",
        "                                  self.temperature)\n",
        "                 for i, p in enumerate(prompts)]\n",
        "        replies = run_batch(lines)\n",
        "        # Preserve original order\n",
        "        return [replies[f\"req-{i}\"] for i in range(len(prompts))]\n",
        "\n",
        "class GPTAPILogprobPipeline(ModelPipelineBase):\n",
        "    \"\"\"\n",
        "    Pipeline to call OpenAI Chat API and record both the parsed answer and its log probability.\n",
        "    \"\"\"\n",
        "    def __init__(self, model: str = \"gpt-3.5-turbo\", temperature: float = 0.0, max_tokens: int = 10):\n",
        "        self.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            raise ValueError(\"OPENAI_API_KEY must be set in environment variables.\")\n",
        "        self.client = OpenAI(api_key=self.api_key)\n",
        "        self.model = model\n",
        "        self.temperature = temperature\n",
        "        self.max_tokens = max_tokens\n",
        "\n",
        "    def call_model(self, prompt: str) -> tuple[str, float]:  # returns (answer, total_logprob)\n",
        "        # Call the Chat API with logprobs\n",
        "        resp = self.client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=self.temperature,\n",
        "            max_tokens=self.max_tokens,\n",
        "            logprobs=True,\n",
        "            top_logprobs=1  # for each token, only the chosen token's logprob\n",
        "        )\n",
        "        choice = resp.choices[0]\n",
        "        text = choice.message.content.strip()\n",
        "        # extract year or fallback\n",
        "        m = re.search(r\"\\b(\\d{4})\\b\", text)\n",
        "        answer = m.group(1) if m else text\n",
        "\n",
        "        # sum of generated token logprobs\n",
        "        token_logps = [tok.logprob for tok in choice.logprobs.content]\n",
        "        total_logp = sum(token_logps)\n",
        "        return answer, total_logp\n",
        "\n",
        "\n",
        "# Factory class to create the appropriate pipeline.\n",
        "class ModelPipelineFactory:\n",
        "    @staticmethod\n",
        "    def create_pipeline(source: str, **kwargs) -> ModelPipelineBase:\n",
        "        \"\"\"\n",
        "        Factory method to instantiate a pipeline based on the provided source.\n",
        "        :param source: A string indicating the source, e.g., \"huggingface\", \"local\", \"api\", or \"gpt\".\n",
        "        :param kwargs: Additional parameters for pipeline initialization.\n",
        "            For HuggingFaceDetailedPipeline:\n",
        "              - model_name (str): Required.\n",
        "              - cache_dir (str): Optional; if not provided and not in environment, no cache_dir is used.\n",
        "              - torch_dtype (torch.dtype): Optional; default torch.float16.\n",
        "              - temperature (float): Optional; default 0.0.\n",
        "              - device (int): Optional; default -1 (CPU).\n",
        "              - max_length (int): Optional; default 150.\n",
        "            For GPTAPIPipeline:\n",
        "              - model (str): Optional; default \"gpt-3.5-turbo\".\n",
        "              - temperature (float): Optional; default 0.0.\n",
        "              - max_tokens (int): Optional; default 150.\n",
        "        :return: An instance of ModelPipelineBase.\n",
        "        \"\"\"\n",
        "        source = source.lower()\n",
        "        if source in {\"huggingface\", \"local\"}:\n",
        "            if \"model_name\" not in kwargs:\n",
        "                raise ValueError(\"model_name must be provided for HuggingFace pipeline.\")\n",
        "            return HuggingFaceDetailedPipeline(\n",
        "                model_name=kwargs[\"model_name\"],\n",
        "                cache_dir=kwargs.get(\"cache_dir\"),\n",
        "                torch_dtype=kwargs.get(\"torch_dtype\", torch.float16),\n",
        "                temperature=kwargs.get(\"temperature\", 0.0),\n",
        "                device=kwargs.get(\"device\", -1),\n",
        "                max_length=kwargs.get(\"max_length\", 150)\n",
        "            )\n",
        "        elif source in {\"api\", \"gpt\"}:\n",
        "            return GPTAPIPipeline(\n",
        "                model=kwargs.get(\"model\", \"gpt-3.5-turbo\"),\n",
        "                temperature=kwargs.get(\"temperature\", 0.0),\n",
        "                max_tokens=kwargs.get(\"max_tokens\", 150)\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported source: {source}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5dCmqrUiaQV"
      },
      "outputs": [],
      "source": [
        "def assemble_initial_prompt(transcript_dict: dict, with_context: bool, agent: str) -> str:\n",
        "    \"\"\"\n",
        "    Assemble a prompt instructing the agent to generate its INITIAL answer.\n",
        "\n",
        "    The prompt includes the question and, if requested, the context.\n",
        "    It then uses the appropriate template from DEBATER, replacing placeholders\n",
        "    for QUESTION, CONTEXT, AGENT, and leaves {ANSWER} as a marker.\n",
        "\n",
        "    Parameters:\n",
        "      transcript_dict (dict): Should contain at least \"question\" and optionally \"context\".\n",
        "      with_context (bool): If True, use the template including context.\n",
        "      agent (str): The agent identifier (e.g., \"A\" or \"B\").\n",
        "\n",
        "    Returns:\n",
        "      A formatted prompt string.\n",
        "    \"\"\"\n",
        "    question = transcript_dict.get(\"question\", \"\")\n",
        "\n",
        "    if with_context:\n",
        "        context = transcript_dict.get(\"context\", \"\")\n",
        "        template = DEBATER.get(\"initial_answer_prompt_with_context\", \"\")\n",
        "        prompt = template.format(QUESTION=question, CONTEXT=context)\n",
        "    else:\n",
        "        template = DEBATER.get(\"initial_answer_prompt_without_context\", \"\")\n",
        "        prompt = template.format(QUESTION=question)\n",
        "\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def assemble_transcript(\n",
        "    transcript_dict: dict,\n",
        "    num_rounds: int,\n",
        "    with_context: bool,\n",
        "    agent: str,\n",
        "    history_mode: str,   # \"full_history\" or \"last_round_history\"\n",
        "    debate_type: str,    # \"self\" or \"both\"\n",
        "    debate_mode: str,    # \"asymmetric\" or \"symmetric\"\n",
        "    role: str = None     # For asymmetric mode: \"challenger\" or \"defender\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Assemble a prompt for a debater.\n",
        "\n",
        "    Parameters:\n",
        "      transcript_dict: contains \"question\", \"context\", initial answers, and round responses.\n",
        "      num_rounds:      number of rounds completed so far.\n",
        "      with_context:    whether to include the context block.\n",
        "      agent:           \"A\" or \"B\".\n",
        "      history_mode:    \"full_history\" or \"last_round_history\".\n",
        "      debate_type:     \"self\" (only own initial) or \"both\" (both agents' initials).\n",
        "      debate_mode:     \"asymmetric\" or \"symmetric\".\n",
        "      role:            for asymmetric: \"challenger\" or \"defender\"; ignored if symmetric.\n",
        "\n",
        "    Returns:\n",
        "      A fully formatted prompt string for the agent.\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "\n",
        "    # 1) Pick the right instruction block based on symmetric/asymmetric + role\n",
        "    if debate_mode.lower() == \"asymmetric\":\n",
        "        if role is None:\n",
        "            instruction = \"Error: Asymmetric debate mode requires a role.\"\n",
        "        elif role.lower() == \"challenger\":\n",
        "            instruction = DEBATER[\"debater_ch_instru\"]\n",
        "        elif role.lower() == \"defender\":\n",
        "            instruction = DEBATER[\"debater_de_instru\"]\n",
        "        else:\n",
        "            instruction = \"Error: Unknown role provided.\"\n",
        "    elif debate_mode.lower() == \"symmetric\":\n",
        "        instruction = DEBATER[\"debater_sym_instru\"]\n",
        "    else:\n",
        "        instruction = \"Error: Unknown debate mode provided.\"\n",
        "\n",
        "    # 1.a) Prepend context‐reliability guidance\n",
        "    context_trust = DEBATER.get(\"context_trust_instru\", \"\")\n",
        "    instruction = f\"{context_trust}\\n\\n{instruction}\"\n",
        "    lines.append(\"Instruction:\")\n",
        "    lines.append(instruction)\n",
        "    lines.append(\"────────────────────────────\")\n",
        "\n",
        "    # 2) Question and optional context\n",
        "    question = transcript_dict.get(\"question\", \"\")\n",
        "    lines.append(f\"Question: {question}\")\n",
        "\n",
        "    if with_context:\n",
        "        context = transcript_dict.get(\"context\", \"\")\n",
        "        if context:\n",
        "            lines.append(f\"Context: {context}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # 3) If no rounds yet, ask for an initial answer\n",
        "    if num_rounds == 0:\n",
        "        lines.append(f\"Agent {agent}, please provide your INITIAL ANSWER based on the above information.\")\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "    # 4) Show initial answers\n",
        "    lines.append(\"Initial Answer(s):\")\n",
        "    if debate_type.lower() == \"self\":\n",
        "        key = f\"debater_{agent}_initial\"\n",
        "        lines.append(f\"  Agent {agent}: {transcript_dict.get(key,'')}\")\n",
        "    elif debate_type.lower() == \"both\":\n",
        "        lines.append(f\"  Agent A: {transcript_dict.get('debater_A_initial','')}\")\n",
        "        lines.append(f\"  Agent B: {transcript_dict.get('debater_B_initial','')}\")\n",
        "    else:\n",
        "        lines.append(\"Error: Unknown debate_type provided.\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # 5) Show previous round(s)\n",
        "    if history_mode.lower() == \"full_history\":\n",
        "        lines.append(\"Previous Round Responses (Full History):\")\n",
        "        for r in range(1, num_rounds):\n",
        "            lines.append(f\"  Round {r}:\")\n",
        "            lines.append(f\"    Agent A: {transcript_dict.get(f'debater_A_round_{r}','')}\")\n",
        "            lines.append(f\"    Agent B: {transcript_dict.get(f'debater_B_round_{r}','')}\")\n",
        "    elif history_mode.lower() == \"last_round_history\":\n",
        "        r = num_rounds\n",
        "        lines.append(\"Previous Round Responses (Last Round):\")\n",
        "        lines.append(f\"  Round {r}:\")\n",
        "        lines.append(f\"    Agent A: {transcript_dict.get(f'debater_A_round_{r}','')}\")\n",
        "        lines.append(f\"    Agent B: {transcript_dict.get(f'debater_B_round_{r}','')}\")\n",
        "    else:\n",
        "        lines.append(\"Error: Unknown history_mode provided.\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # 6) End‐of‐round thinking prompt\n",
        "    if num_rounds == 1:\n",
        "        end_instr = DEBATER[\"first_round_thinking\"]\n",
        "    elif num_rounds == 2:\n",
        "        end_instr = DEBATER[\"second_round_thinking\"]\n",
        "    else:\n",
        "        end_instr = DEBATER[\"nth_round_thinking\"]\n",
        "\n",
        "    lines.append(\"End Instruction:\")\n",
        "    lines.append(end_instr)\n",
        "    lines.append(\"\")\n",
        "    lines.append(f\"Agent {agent}, please provide your NEXT RESPONSE based on the above conversation.\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def assemble_judge_prompt(\n",
        "    transcript_dict: dict,\n",
        "    num_rounds: int,\n",
        "    with_context: bool,\n",
        "    judge_type: str = \"symmetric\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Assemble a prompt for the judge agent based on the debate transcript dictionary.\n",
        "\n",
        "    The prompt includes:\n",
        "      - A judge-specific instruction from JUDGE (selected based on judge_type),\n",
        "      - The complete debate transcript, including the question, (optionally) context, initial answers, and all round responses.\n",
        "      - A final instruction to generate the final decision, which is provided by the JUDGE template.\n",
        "\n",
        "    Parameters:\n",
        "      transcript_dict (dict): Contains \"question\", \"context\", initial answers, and round responses.\n",
        "      num_rounds (int): Number of completed debate rounds.\n",
        "      with_context (bool): Whether to include context.\n",
        "      judge_type (str): \"symmetric\" or \"asymmetric\" to select the appropriate judge instruction.\n",
        "\n",
        "    Returns:\n",
        "      A formatted prompt string for the judge.\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "\n",
        "    # Select judge instruction.\n",
        "    if judge_type.lower() == \"symmetric\":\n",
        "        judge_instruction = JUDGE.get(\"judge_sym_instru\", \"\")\n",
        "    elif judge_type.lower() == \"asymmetric\":\n",
        "        judge_instruction = JUDGE.get(\"judge_asym_instru\", \"\")\n",
        "    else:\n",
        "        judge_instruction = \"Error: Unknown judge type provided.\"\n",
        "\n",
        "    lines.append(\"Judge Instruction:\")\n",
        "    lines.append(judge_instruction)\n",
        "    lines.append(\"────────────────────────────\")\n",
        "\n",
        "    # Add the question.\n",
        "    question = transcript_dict.get(\"question\", \"\")\n",
        "    lines.append(\"Question: {QUESTION}\".format(QUESTION=question))\n",
        "\n",
        "    # Optionally, add context.\n",
        "    if with_context:\n",
        "        context = transcript_dict.get(\"context\", \"\")\n",
        "        if context:\n",
        "            lines.append(\"Context: {CONTEXT}\".format(CONTEXT=context))\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Add initial answers.\n",
        "    lines.append(\"Initial Answers:\")\n",
        "    lines.append(\"  Agent A: \" + transcript_dict.get(\"debater_A_initial\", \"\"))\n",
        "    lines.append(\"  Agent B: \" + transcript_dict.get(\"debater_B_initial\", \"\"))\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Add round responses (full history is preferred for the judge).\n",
        "    if num_rounds > 0:\n",
        "        lines.append(\"Round Responses:\")\n",
        "        for r in range(1, num_rounds):\n",
        "            lines.append(\"  Round {r}:\".format(r=r))\n",
        "            lines.append(\"    Agent A: \" + transcript_dict.get(f\"debater_A_round_{r}\", \"\"))\n",
        "            lines.append(\"    Agent B: \" + transcript_dict.get(f\"debater_B_round_{r}\", \"\"))\n",
        "    else:\n",
        "        lines.append(\"No debate rounds have been completed yet.\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Use the final decision prompt from JUDGE.\n",
        "    final_instr = JUDGE.get(\"final_decision_prompt\", \"\")\n",
        "    lines.append(final_instr)\n",
        "\n",
        "    return \"\\n\".join(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4vSXTVMhiPa"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Optional\n",
        "\n",
        "class Debater:\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str,\n",
        "        pipeline: ModelPipelineBase,\n",
        "        has_context: bool = False,\n",
        "        context: Optional[str] = None,\n",
        "        min_words: int = 5,\n",
        "        max_words: int = 50,\n",
        "        debate_mode: str = \"symmetric\",\n",
        "        debate_type: str = \"both\",\n",
        "        history_mode: str = \"full_history\",\n",
        "        role: Optional[str] = None,\n",
        "    ):\n",
        "        self.name          = name\n",
        "        self.pipeline      = pipeline\n",
        "        self.has_context   = has_context\n",
        "        self.context       = context\n",
        "        self.min_words     = min_words\n",
        "        self.max_words     = max_words\n",
        "        self.debate_mode   = debate_mode       # \"symmetric\" or \"asymmetric\"\n",
        "        self.debate_type = debate_type     # \"self\" or \"both\"\n",
        "        self.history_mode  = history_mode      # \"full_history\" or \"last_round_history\"\n",
        "        self.role          = role              # only used if asym\n",
        "\n",
        "    def get_initial_answer(self, transcript_dict: dict) -> str:\n",
        "        prompt  = assemble_initial_prompt(transcript_dict,\n",
        "                                         with_context=self.has_context,\n",
        "                                         agent=self.name)\n",
        "        response = self.pipeline.call_model(prompt)\n",
        "        # extract after \"answer:\"\n",
        "        #m = re.search(r\"answer:\\s*(.*)\", response, re.IGNORECASE)\n",
        "        #return m.group(1).strip() if m else response\n",
        "        return response\n",
        "\n",
        "    def debate_response(self, transcript_dict: dict, round_num: int) -> str:\n",
        "        prompt = assemble_transcript(\n",
        "            transcript_dict,\n",
        "            num_rounds=round_num,\n",
        "            with_context=self.has_context,\n",
        "            agent=self.name,\n",
        "            history_mode=self.history_mode,\n",
        "            debate_type=self.debate_type,   # “self” or “both”\n",
        "            debate_mode=self.debate_mode,   # “symmetric” or “asymmetric”\n",
        "            role=self.role,\n",
        "        )\n",
        "        return self.pipeline.call_model(prompt)\n",
        "\n",
        "    def extract_argument(self, response: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract the text within <argument>...</argument> tags.\n",
        "        \"\"\"\n",
        "        match = re.search(r\"<argument>(.*?)</argument>\", response)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "        return response.strip()\n",
        "\n",
        "    def truncate(self, argument: str) -> str:\n",
        "        \"\"\"\n",
        "        Truncate the argument to the maximum word count.\n",
        "        \"\"\"\n",
        "        words = argument.split()\n",
        "        if len(words) > self.max_words:\n",
        "            truncated = \" \".join(words[:self.max_words]) + \"... <TRUNCATED>\"\n",
        "            return truncated\n",
        "        return argument\n",
        "\n",
        "\n",
        "class Judge:\n",
        "    def __init__(self, name: str, pipeline: ModelPipelineBase):\n",
        "        \"\"\"\n",
        "        Initialize the Judge.\n",
        "        \"\"\"\n",
        "        self.name = name\n",
        "        self.pipeline = pipeline\n",
        "\n",
        "    def decide(self, transcript: str) -> str:\n",
        "        \"\"\"\n",
        "        Generate a final decision based on the debate transcript.\n",
        "        Uses the appropriate judge instruction from the JUDGE template.\n",
        "        \"\"\"\n",
        "        prompt = JUDGE.get(\"judge_sym_instru\", \"\").format(name=self.name, transcript=transcript)\n",
        "        response = self.pipeline.call_model(prompt)\n",
        "        decision = self.extract_clean_argument(response)\n",
        "        return f\"[{self.name}] Final Decision: {decision}\"\n",
        "\n",
        "    def extract_clean_argument(self, response: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract the text within <argument>...</argument> tags from the judge's response.\n",
        "        \"\"\"\n",
        "        match = re.search(r\"<argument>(.*?)</argument>\", response)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "        return response.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LhdCdgYs8VH"
      },
      "outputs": [],
      "source": [
        "def _make_jsonl_line(custom_id, prompt, max_tokens, temp=0.0):\n",
        "    body = {\"model\": MODEL_ID,\n",
        "            \"messages\":[{\"role\":\"user\",\"content\":prompt}],\n",
        "            \"max_tokens\":max_tokens,\"temperature\":temp}\n",
        "    return json.dumps({\"custom_id\":custom_id,\"method\":\"POST\",\n",
        "                       \"url\":\"/v1/chat/completions\",\"body\":body},\n",
        "                       ensure_ascii=False)\n",
        "\n",
        "def _upload_and_batch(lines):\n",
        "    fn = tempfile.mktemp(suffix=\".jsonl\")\n",
        "    Path(fn).write_text(\"\\n\".join(lines),encoding=\"utf-8\")\n",
        "    file_id = client.files.create(file=open(fn,\"rb\"), purpose=\"batch\").id\n",
        "    batch   = client.batches.create(input_file_id=file_id,\n",
        "                                    endpoint=\"/v1/chat/completions\",\n",
        "                                    completion_window=\"24h\")\n",
        "    while batch.status not in {\"completed\",\"failed\",\"expired\",\"cancelled\"}:\n",
        "        time.sleep(10)\n",
        "        batch = client.batches.retrieve(batch.id)\n",
        "    txt = client.files.content(batch.output_file_id).text if batch.output_file_id else \"\"\n",
        "    out={}\n",
        "    for ln in txt.strip().splitlines():\n",
        "        rec=json.loads(ln)\n",
        "        out[rec[\"custom_id\"]] = (rec[\"response\"][\"body\"][\"choices\"][0]\n",
        "                                  [\"message\"][\"content\"]\n",
        "                                  if rec[\"error\"] is None else\n",
        "                                  f\"[ERROR:{rec['error']['code']}]\")\n",
        "    return out\n",
        "\n",
        "# shortcut\n",
        "def L(cid,prompt,max_tokens=250,temp=0.0): return _make_jsonl_line(cid,prompt,max_tokens,temp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvgeFvT7B58n"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "YEAR_RE = re.compile(r\"\\b(\\d{4})\\b\")\n",
        "def year(txt): m=YEAR_RE.search(txt); return m.group(1) if m else txt.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIUsGlcwB934"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "def run_condition(order: str, total_rounds: int = 3) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    order ∈ {\"A_first\", \"B_first\", \"simul\"}\n",
        "    total_rounds: how many debate rounds AFTER round-0\n",
        "    Returns tasks with prompts/answers plus judge verdicts for EVERY round.\n",
        "    \"\"\"\n",
        "    tasks = deepcopy(tasks_base)\n",
        "\n",
        "    # Round-0 independent answers\n",
        "    batch = []\n",
        "    for i, t in enumerate(tasks):\n",
        "        pA = INIT_CTX.format(QUESTION=t[\"question\"], CONTEXT=t[\"context\"])\n",
        "        pB = INIT_NO .format(QUESTION=t[\"question\"])\n",
        "        t[\"prompt_A0\"], t[\"prompt_B0\"] = pA, pB\n",
        "        batch.append(L(f\"A0-{i}\", pA, 10))\n",
        "        batch.append(L(f\"B0-{i}\", pB, 10))\n",
        "    res = _upload_and_batch(batch)\n",
        "    for i, t in enumerate(tasks):\n",
        "        t[\"A0\"] = year(res[f\"A0-{i}\"])\n",
        "        t[\"B0\"] = year(res[f\"B0-{i}\"])\n",
        "\n",
        "    # helper to craft prompts\n",
        "    def make_prompt(agent: str, t: dict, r: int) -> str:\n",
        "        opp = \"B\" if agent == \"A\" else \"A\"\n",
        "        ctx = CTX_BLK.format(CONTEXT=t[\"context\"]) if agent == \"A\" else \"\"\n",
        "        base = INTRO.format(NAME=agent, QUESTION=t[\"question\"],\n",
        "                            CTXBLOCK=ctx, ANS=t[f\"{agent}0\"])\n",
        "\n",
        "        if r == 1:                                     # opening debate\n",
        "            first = (order == \"A_first\" and agent == \"A\") or \\\n",
        "                    (order == \"B_first\" and agent == \"B\") or \\\n",
        "                    (order == \"simul\")\n",
        "            if first:\n",
        "                return base + ROLE_ATTACK\n",
        "            return base + f\"Opponent’s answer: {t[f'{opp}1']}\\n\\n\" + ROLE_ATTACK\n",
        "\n",
        "        # r ≥ 2\n",
        "        hist = \"\".join(\n",
        "            f\"Round {k}: A:{t[f'A{k}']} B:{t[f'B{k}']}\\n\" for k in range(1, r)\n",
        "        )\n",
        "        first = (order == \"A_first\" and agent == \"A\") or \\\n",
        "                (order == \"B_first\" and agent == \"B\")\n",
        "        if order == \"simul\":\n",
        "            opp_line = f\"Opponent’s last answer: {t[f'{opp}{r-1}']}\\n\\n\"\n",
        "        elif first:\n",
        "            opp_line = \"\"\n",
        "        else:\n",
        "            opp_line = f\"Opponent’s current answer: {t[f'{opp}{r}']}\\n\\n\"\n",
        "\n",
        "        return base + ROUND_HEAD.format(R=r, HIST=hist) + opp_line + ROLE_DEFEND\n",
        "\n",
        "    # loop over debate rounds\n",
        "    for r in range(1, total_rounds + 1):\n",
        "        # produce A_r and B_r\n",
        "        if order == \"simul\":\n",
        "            lines = []\n",
        "            for i, t in enumerate(tasks):\n",
        "                pA = make_prompt(\"A\", t, r); t[f\"prompt_A{r}\"] = pA\n",
        "                pB = make_prompt(\"B\", t, r); t[f\"prompt_B{r}\"] = pB\n",
        "                lines += [L(f\"A{r}-{i}\", pA), L(f\"B{r}-{i}\", pB)]\n",
        "            res = _upload_and_batch(lines)\n",
        "            for i, t in enumerate(tasks):\n",
        "                t[f\"A{r}\"] = res[f\"A{r}-{i}\"]\n",
        "                t[f\"B{r}\"] = res[f\"B{r}-{i}\"]\n",
        "\n",
        "        elif order == \"A_first\":\n",
        "            lines = [L(f\"A{r}-{i}\", make_prompt(\"A\", t, r)) for i, t in enumerate(tasks)]\n",
        "            res = _upload_and_batch(lines)\n",
        "            for i, t in enumerate(tasks):\n",
        "                t[f\"A{r}\"] = res[f\"A{r}-{i}\"]\n",
        "\n",
        "            lines = [L(f\"B{r}-{i}\", make_prompt(\"B\", t, r)) for i, t in enumerate(tasks)]\n",
        "            res = _upload_and_batch(lines)\n",
        "            for i, t in enumerate(tasks):\n",
        "                t[f\"B{r}\"] = res[f\"B{r}-{i}\"]\n",
        "\n",
        "        else:  # B_first\n",
        "            lines = [L(f\"B{r}-{i}\", make_prompt(\"B\", t, r)) for i, t in enumerate(tasks)]\n",
        "            res = _upload_and_batch(lines)\n",
        "            for i, t in enumerate(tasks):\n",
        "                t[f\"B{r}\"] = res[f\"B{r}-{i}\"]\n",
        "\n",
        "            lines = [L(f\"A{r}-{i}\", make_prompt(\"A\", t, r)) for i, t in enumerate(tasks)]\n",
        "            res = _upload_and_batch(lines)\n",
        "            for i, t in enumerate(tasks):\n",
        "                t[f\"A{r}\"] = res[f\"A{r}-{i}\"]\n",
        "\n",
        "        print(f\"{order}: finished round {r}\")\n",
        "\n",
        "        # judge after this round\n",
        "        jlines = []\n",
        "        for i, t in enumerate(tasks):\n",
        "            transcript = (\n",
        "                f\"Round 0: A={t['A0']}  B={t['B0']}\\n\" +\n",
        "                \"\".join(\n",
        "                    f\"Round {k}: A:{t[f'A{k}']}  B:{t[f'B{k}']}\\n\"\n",
        "                    for k in range(1, r + 1)\n",
        "                )\n",
        "            )\n",
        "            jprompt = (\n",
        "                f\"Question: {t['question']}\\n\"\n",
        "                f\"Transcript up to round {r}:\\n{transcript}\\n\\n\"\n",
        "                f\"{JUDGE_TAIL}\"\n",
        "            )\n",
        "            t[f\"judge_r{r}_prompt\"] = jprompt\n",
        "            jlines.append(L(f\"J{r}-{i}\", jprompt, TOK_JUDGE))\n",
        "        jres = _upload_and_batch(jlines)\n",
        "        for i, t in enumerate(tasks):\n",
        "            raw = jres[f\"J{r}-{i}\"]\n",
        "            t[f\"judge_r{r}_output\"] = raw\n",
        "            t[f\"judge_r{r}_year\"] = year(raw)\n",
        "\n",
        "    return tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWTb0jW3B7gO"
      },
      "outputs": [],
      "source": [
        "\n",
        "MODEL_ID   = \"\"\n",
        "DATA_FILE  = Path(\"\")\n",
        "tasks_base = json.load(DATA_FILE.open())\n",
        "#tasks_base[:10]\n",
        "ROUNDS     = 3\n",
        "ORDER_MODES= [\"A_first\",\"B_first\",\"simul\"]\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "TOK_JUDGE = 200\n",
        "YEAR_RE    = re.compile(r\"\\b(\\d{4})\\b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQpxE0eB0hgK",
        "outputId": "abf4d7b1-974c-420e-d706-4106317395cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A_first : finished round 1\n",
            "A_first : finished round 2\n",
            "A_first : finished round 3\n",
            "✅ saved judge_debate_A_first_R3_gpt-3.5-turbo_20250524_123908.json\n",
            "B_first : finished round 1\n",
            "B_first : finished round 2\n",
            "B_first : finished round 3\n",
            "✅ saved judge_debate_B_first_R3_gpt-3.5-turbo_20250524_152618.json\n",
            "simul : finished round 1\n",
            "simul : finished round 2\n",
            "simul : finished round 3\n",
            "✅ saved judge_debate_simul_R3_gpt-3.5-turbo_20250524_170353.json\n"
          ]
        }
      ],
      "source": [
        "# ---------- 6.  Run all three modes & save --------------------\n",
        "name_list = []\n",
        "for mode in ORDER_MODES:\n",
        "    result=run_condition(mode)\n",
        "    ts=datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    fname=f\"judge_debate_{mode}_R{ROUNDS}_{MODEL_ID}_{ts}.json\"\n",
        "    name_list.append(fname)\n",
        "    json.dump(result,(Path(\".\")/fname).open(\"w\",encoding=\"utf-8\"),indent=2)\n",
        "    print(\"✅ saved\", fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ThKjM6sQphtL",
        "outputId": "d1dfe102-618c-4cc4-df42-d2ad286d4e12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ wrote judge_debate_A_first_R3_gpt-3.5-turbo_20250524_123908_J12.json\n",
            "✅ wrote judge_debate_B_first_R3_gpt-3.5-turbo_20250524_152618_J12.json\n",
            "✅ wrote judge_debate_simul_R3_gpt-3.5-turbo_20250524_170353_J12.json\n"
          ]
        }
      ],
      "source": [
        "import json, re, os, time, tempfile\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any\n",
        "from openai import OpenAI\n",
        "\n",
        "number_rounds = 2\n",
        "\n",
        "def year(txt:str)->str: m=YEAR_RE.search(txt); return m.group(1) if m else txt.strip()\n",
        "\n",
        "FILES = name_list\n",
        "FILES = [Path(f) for f in FILES]\n",
        "\n",
        "# Batch helpers (same as before)\n",
        "client = OpenAI()\n",
        "\n",
        "def make_line(cid,prompt,max_tok=TOK_JUDGE,temp=0.0):\n",
        "    body={\"model\":MODEL_ID,\"messages\":[{\"role\":\"user\",\"content\":prompt}],\n",
        "          \"max_tokens\":max_tok,\"temperature\":temp}\n",
        "    return json.dumps({\"custom_id\":cid,\"method\":\"POST\",\n",
        "                       \"url\":\"/v1/chat/completions\",\"body\":body},\n",
        "                      ensure_ascii=False)\n",
        "\n",
        "def batch_run(lines):\n",
        "    fn=tempfile.mktemp(suffix=\".jsonl\")\n",
        "    Path(fn).write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
        "    fid=client.files.create(file=open(fn,\"rb\"),purpose=\"batch\").id\n",
        "    bid=client.batches.create(input_file_id=fid,endpoint=\"/v1/chat/completions\",\n",
        "                              completion_window=\"24h\").id\n",
        "    while True:\n",
        "        b=client.batches.retrieve(bid)\n",
        "        if b.status in {\"completed\",\"failed\",\"expired\",\"cancelled\"}:\n",
        "            break\n",
        "        time.sleep(8)\n",
        "    txt=client.files.content(b.output_file_id).text if b.output_file_id else \"\"\n",
        "    out={}\n",
        "    for ln in txt.strip().splitlines():\n",
        "        rec=json.loads(ln)\n",
        "        out[rec[\"custom_id\"]]=(rec[\"response\"][\"body\"][\"choices\"][0]\n",
        "                               [\"message\"][\"content\"]\n",
        "                               if rec[\"error\"] is None else\n",
        "                               f\"[ERROR:{rec['error']['code']}]\")\n",
        "    return out\n",
        "\n",
        "\n",
        "for fpath in FILES:\n",
        "    tasks=json.loads(fpath.read_text())\n",
        "\n",
        "    for r in (1,number_rounds):\n",
        "        # build all prompts for this round\n",
        "        lines=[]\n",
        "        for i,t in enumerate(tasks):\n",
        "            transcript = (\n",
        "                f\"Round 0: A={t['A0']}  B={t['B0']}\\n\" +\n",
        "                \"\".join(\n",
        "                    f\"Round {k}: A:{t[f'A{k}']}  B:{t[f'B{k}']}\\n\"\n",
        "                    for k in range(1, r + 1)\n",
        "                )\n",
        "            )\n",
        "            jp=(f\"Question: {t['question']}\\n\"\n",
        "                f\"Debate transcript (up to round {r}):\\n{transcript}\\n\\n\"\n",
        "                f\"{JUDGE_TAIL}\")\n",
        "            t[f\"judge_r{r}_prompt\"]=jp      # store prompt\n",
        "            lines.append(make_line(f\"J{r}-{i}\", jp))\n",
        "        # batch run\n",
        "        res=batch_run(lines)\n",
        "        for i,t in enumerate(tasks):\n",
        "            raw=res[f\"J{r}-{i}\"]\n",
        "            t[f\"judge_r{r}_output\"]=raw\n",
        "            t[f\"judge_r{r}_year\"]=year(raw)\n",
        "\n",
        "    # save new file\n",
        "    out_name=fpath.with_name(fpath.stem+\"_J12.json\")\n",
        "    out_name.write_text(json.dumps(tasks,indent=2,ensure_ascii=False))\n",
        "    print(\"✅ wrote\", out_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnLxkK0WL970"
      },
      "source": [
        "#### R3 to R5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmmGlAiaL5aw",
        "outputId": "24072347-7ac2-4b89-8135-c02bd9afdd6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "simul : finished new round 4\n",
            "simul : finished new round 5\n",
            "✅ wrote judge_debate_simul_R5_gpt-3.5-turbo_20250520_200713_J12_J12345.json\n",
            "A_first : finished new round 4\n",
            "A_first : finished new round 5\n",
            "✅ wrote judge_debate_A_first_R5_gpt-3.5-turbo_20250520_134000_J12_J12345.json\n",
            "B_first : finished new round 4\n",
            "B_first : finished new round 5\n",
            "✅ wrote judge_debate_B_first_R5_gpt-3.5-turbo_20250520_162208_J12_J12345.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#  Extend existing Rn debate files to full m rounds + judges\n",
        "\n",
        "import json, re, os, time, tempfile\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from copy import deepcopy\n",
        "from openai import OpenAI\n",
        "\n",
        "# config\n",
        "MODEL_ID   = \"gpt-3.5-turbo\"\n",
        "TOK_DEB    = 250\n",
        "TOK_JUDGE  = 120\n",
        "NEW_ROUNDS = []      # add these rounds\n",
        "YEAR_RE    = re.compile(r\"\\b(\\d{4})\\b\")\n",
        "def year(x): m=YEAR_RE.search(str(x)); return m.group(1) if m else str(x).strip()\n",
        "\n",
        "\n",
        "FILES = {\n",
        "\n",
        "}\n",
        "FILES = [Path(f) for f in FILES]\n",
        "\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "def make_line(cid, prompt, max_tok, temp=0.0):\n",
        "    body={\"model\":MODEL_ID,\"messages\":[{\"role\":\"user\",\"content\":prompt}],\n",
        "          \"max_tokens\":max_tok,\"temperature\":temp}\n",
        "    return json.dumps({\"custom_id\":cid,\"method\":\"POST\",\n",
        "                       \"url\":\"/v1/chat/completions\",\"body\":body},\n",
        "                      ensure_ascii=False)\n",
        "\n",
        "def batch_run(lines):\n",
        "    fn=tempfile.mktemp(suffix=\".jsonl\")\n",
        "    Path(fn).write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
        "    fid=client.files.create(file=open(fn,\"rb\"),purpose=\"batch\").id\n",
        "    bid=client.batches.create(input_file_id=fid,endpoint=\"/v1/chat/completions\",\n",
        "                              completion_window=\"24h\").id\n",
        "    while True:\n",
        "        b=client.batches.retrieve(bid)\n",
        "        if b.status in {\"completed\",\"failed\",\"expired\",\"cancelled\"}:\n",
        "            break\n",
        "        time.sleep(8)\n",
        "    txt=client.files.content(b.output_file_id).text if b.output_file_id else \"\"\n",
        "    out={}\n",
        "    for ln in txt.strip().splitlines():\n",
        "        rec=json.loads(ln)\n",
        "        out[rec[\"custom_id\"]]=(rec[\"response\"][\"body\"][\"choices\"][0]\n",
        "                               [\"message\"][\"content\"]\n",
        "                               if rec[\"error\"] is None else\n",
        "                               f\"[ERROR:{rec['error']['code']}]\")\n",
        "    return out\n",
        "\n",
        "\n",
        "for fpath in FILES:\n",
        "    mode = (\"A_first\" if \"A_first\" in fpath.name else\n",
        "            \"B_first\" if \"B_first\" in fpath.name else \"simul\")\n",
        "    tasks = json.loads(fpath.read_text())\n",
        "\n",
        "    def make_prompt(agent:str, t:dict, r:int):\n",
        "        opp   = \"B\" if agent==\"A\" else \"A\"\n",
        "        ctx   = CTX_BLK.format(CONTEXT=t[\"context\"]) if agent==\"A\" else \"\"\n",
        "        base  = INTRO.format(NAME=agent, QUESTION=t[\"question\"],\n",
        "                             CTXBLOCK=ctx, ANS=t[f\"{agent}0\"])\n",
        "        hist  = \"\".join(f\"Round {k}: A:{t[f'A{k}']}  B:{t[f'B{k}']}\\n\"\n",
        "                        for k in range(1, r))\n",
        "        first = (mode==\"A_first\" and agent==\"A\") or \\\n",
        "                (mode==\"B_first\" and agent==\"B\")\n",
        "        if mode==\"simul\":\n",
        "            opp_line = f\"Opponent’s last answer: {t[f'{opp}{r-1}']}\\n\\n\"\n",
        "        elif first:\n",
        "            opp_line = \"\"     # first speaker doesn't see current-round opp\n",
        "        else:\n",
        "            opp_line = f\"Opponent’s current answer: {t[f'{opp}{r}']}\\n\\n\"\n",
        "        return base + ROUND_HEAD.format(R=r, HIST=hist) + opp_line + ROLE_DEFEND\n",
        "\n",
        "    for r in NEW_ROUNDS:\n",
        "        if mode==\"simul\":\n",
        "            batch=[]\n",
        "            for i,t in enumerate(tasks):\n",
        "                pA=make_prompt(\"A\",t,r); t[f\"prompt_A{r}\"]=pA\n",
        "                pB=make_prompt(\"B\",t,r); t[f\"prompt_B{r}\"]=pB\n",
        "                batch.append(make_line(f\"A{r}-{i}\", pA, TOK_DEB))\n",
        "                batch.append(make_line(f\"B{r}-{i}\", pB, TOK_DEB))\n",
        "            res=batch_run(batch)\n",
        "            for i,t in enumerate(tasks):\n",
        "                t[f\"A{r}\"]=res[f\"A{r}-{i}\"]; t[f\"B{r}\"]=res[f\"B{r}-{i}\"]\n",
        "\n",
        "        elif mode==\"A_first\":\n",
        "            # A speaks\n",
        "            batch=[make_line(f\"A{r}-{i}\", make_prompt(\"A\",t,r), TOK_DEB)\n",
        "                   for i,t in enumerate(tasks)]\n",
        "            res=batch_run(batch)\n",
        "            for i,t in enumerate(tasks): t[f\"A{r}\"]=res[f\"A{r}-{i}\"]\n",
        "            # B replies\n",
        "            batch=[make_line(f\"B{r}-{i}\", make_prompt(\"B\",t,r), TOK_DEB)\n",
        "                   for i,t in enumerate(tasks)]\n",
        "            res=batch_run(batch)\n",
        "            for i,t in enumerate(tasks): t[f\"B{r}\"]=res[f\"B{r}-{i}\"]\n",
        "\n",
        "        else:  # B_first\n",
        "            batch=[make_line(f\"B{r}-{i}\", make_prompt(\"B\",t,r), TOK_DEB)\n",
        "                   for i,t in enumerate(tasks)]\n",
        "            res=batch_run(batch)\n",
        "            for i,t in enumerate(tasks): t[f\"B{r}\"]=res[f\"B{r}-{i}\"]\n",
        "            batch=[make_line(f\"A{r}-{i}\", make_prompt(\"A\",t,r), TOK_DEB)\n",
        "                   for i,t in enumerate(tasks)]\n",
        "            res=batch_run(batch)\n",
        "            for i,t in enumerate(tasks): t[f\"A{r}\"]=res[f\"A{r}-{i}\"]\n",
        "\n",
        "        print(mode,\": finished new round\", r)\n",
        "\n",
        "\n",
        "        jlines=[]\n",
        "        for i,t in enumerate(tasks):\n",
        "            transcript=\"Round 0: A=\"+t[\"A0\"]+\"  B=\"+t[\"B0\"]+\"\\n\"+ \\\n",
        "                       \"\".join(f\"Round {k}: A:{t[f'A{k}']}  B:{t[f'B{k}']}\\n\"\n",
        "                               for k in range(1,r+1))\n",
        "            jp=(f\"Question: {t['question']}\\n\"\n",
        "                f\"Transcript up to round {r}:\\n{transcript}\\n\\n\"\n",
        "                f\"{JUDGE_TAIL}\")\n",
        "            t[f\"judge_r{r}_prompt\"]=jp\n",
        "            jlines.append(make_line(f\"J{r}-{i}\", jp, TOK_JUDGE))\n",
        "        res=batch_run(jlines)\n",
        "        for i,t in enumerate(tasks):\n",
        "            raw=res[f\"J{r}-{i}\"]\n",
        "            t[f\"judge_r{r}_output\"]=raw\n",
        "            t[f\"judge_r{r}_year\"]=year(raw)\n",
        "\n",
        "    out_name = fpath.with_name(fpath.stem+\"_extend.json\")\n",
        "    out_name.write_text(json.dumps(tasks, indent=2, ensure_ascii=False))\n",
        "    print(\"✅ wrote\", out_name)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}