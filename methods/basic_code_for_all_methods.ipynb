{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myvhYB1ejlwt"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import re\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im_9GeYmvZCk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import abc\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import math\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import requests\n",
        "#from mistralai import Mistral\n",
        "from itertools import islice\n",
        "import time\n",
        "from typing import List\n",
        "#from anthropic import Client as AnthropicClient\n",
        "#import anthropic\n",
        "import threading\n",
        "from collections import deque\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def chunkify(lst, n):\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i : i + n]\n",
        "\n",
        "# Base abstract class for model pipelines.\n",
        "class ModelPipelineBase(abc.ABC):\n",
        "    @abc.abstractmethod\n",
        "    def call_model(self, prompt: str) -> str:\n",
        "        \"\"\"Send a prompt to the model and return its response.\"\"\"\n",
        "        pass\n",
        "    def call_model_batch(self, prompts: list[str], batch_size: int = 16) -> list[str]:\n",
        "        \"\"\"\n",
        "        Split `prompts` into chunks of size batch_size, then call `call_model`\n",
        "        on each chunk in parallel threads.\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        for chunk in chunkify(prompts, batch_size):\n",
        "            with ThreadPoolExecutor(max_workers=len(chunk)) as ex:\n",
        "                futures = [ex.submit(self.call_model, p) for p in chunk]\n",
        "                for f in futures:\n",
        "                    results.append(f.result())\n",
        "        return results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from openai import OpenAI, APIError\n",
        "\n",
        "class LLMAPIPipeline(ModelPipelineBase):\n",
        "    \"\"\"\n",
        "    OpenAI-compatible pipeline for https://api.llmapi.com.\n",
        "    Rate-limited to 200 requests/min. Multiple completions (n>1) count as\n",
        "    multiple requests.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: str = \"llama3.1-70b\",\n",
        "        temperature: float = 0.0,\n",
        "        max_tokens: int = 256,\n",
        "        top_p: float | None = None,\n",
        "    ):\n",
        "        self.client = OpenAI(\n",
        "            api_key = os.getenv(\"LLMAPI_TOKEN\"),\n",
        "            base_url = LLMAPI_BASE_URL      # OpenAI client supports custom base_url  :contentReference[oaicite:3]{index=3}\n",
        "        )\n",
        "        if not self.client.api_key:\n",
        "            raise ValueError(\"LLMAPI_TOKEN env var or api_key param is required.\")\n",
        "        self.model        = model\n",
        "        self.temperature  = temperature\n",
        "        self.max_tokens   = max_tokens\n",
        "        self.top_p        = top_p\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Single prompt → single completion\n",
        "    # ------------------------------------------------------------------\n",
        "    def call_model(self, prompt: str, n: int = 1) -> str | list[str]:\n",
        "        \"\"\"Wrap the direct client call in up to 3 retries on APIError.\"\"\"\n",
        "        attempts = 0\n",
        "        while True:\n",
        "            try:\n",
        "                completion = self.client.chat.completions.create(\n",
        "                    model       = self.model,\n",
        "                    messages    = [{\"role\": \"user\", \"content\": prompt}],\n",
        "                    temperature = self.temperature,\n",
        "                    max_tokens  = self.max_tokens,\n",
        "                    n           = n,\n",
        "                    top_p       = self.top_p,\n",
        "                )\n",
        "                texts = [c.message.content for c in completion.choices]\n",
        "                return texts[0] if n == 1 else texts\n",
        "\n",
        "            except APIError as e:\n",
        "                attempts += 1\n",
        "                if attempts >= 10:\n",
        "                    # give up after 3 tries\n",
        "                    raise\n",
        "                # back off and retry\n",
        "                time.sleep(1)\n",
        "\n",
        "    def call_model_batch(self, prompts: list[str], batch_size: int = 16, n: int = 1) -> list[str]:\n",
        "        \"\"\"\n",
        "        Parallel batch calls with:\n",
        "          - 200 req/min rate limit\n",
        "          - up to 3 retries on any APIError\n",
        "        \"\"\"\n",
        "        timestamps = deque()\n",
        "        lock = threading.Lock()\n",
        "\n",
        "        def rate_limit(count: int = 1):\n",
        "            with lock:\n",
        "                now = time.time()\n",
        "                while timestamps and now - timestamps[0] > 60:\n",
        "                    timestamps.popleft()\n",
        "                if len(timestamps) + count > 200:\n",
        "                    sleep_for = 60 - (now - timestamps[0]) + 0.05\n",
        "                    time.sleep(sleep_for)\n",
        "                    now = time.time()\n",
        "                    while timestamps and now - timestamps[0] > 60:\n",
        "                        timestamps.popleft()\n",
        "                for _ in range(count):\n",
        "                    timestamps.append(now)\n",
        "\n",
        "        def safe_call(prompt: str, n: int):\n",
        "            attempts = 0\n",
        "            while True:\n",
        "                rate_limit(count=n)\n",
        "                try:\n",
        "                    result = self.call_model(prompt, n=n)\n",
        "                    return [result] if n == 1 else result\n",
        "                except APIError:\n",
        "                    attempts += 1\n",
        "                    print(f\"APIError on prompt: {prompt}. Retrying... (attempt {attempts}/3)\")\n",
        "                    if attempts >= 10:\n",
        "                        raise\n",
        "                    time.sleep(1)\n",
        "\n",
        "        results = []\n",
        "        for chunk in chunkify(prompts, batch_size):\n",
        "            with ThreadPoolExecutor(max_workers=len(chunk)) as ex:\n",
        "                futures = [ex.submit(safe_call, p, n) for p in chunk]\n",
        "                for f in futures:\n",
        "                    results.extend(f.result())\n",
        "\n",
        "        return results\n",
        "# Factory class to create the appropriate pipeline.\n",
        "class ModelPipelineFactory:\n",
        "    @staticmethod\n",
        "    def create_pipeline(source: str, **kwargs) -> ModelPipelineBase:\n",
        "        \"\"\"\n",
        "        Factory method to instantiate a pipeline based on the provided source.\n",
        "        :param source: A string indicating the source, e.g., \"huggingface\", \"local\", \"api\", or \"gpt\".\n",
        "        :param kwargs: Additional parameters for pipeline initialization.\n",
        "            For HuggingFaceDetailedPipeline:\n",
        "              - model_name (str): Required.\n",
        "              - cache_dir (str): Optional; if not provided and not in environment, no cache_dir is used.\n",
        "              - torch_dtype (torch.dtype): Optional; default torch.float16.\n",
        "              - temperature (float): Optional; default 0.0.\n",
        "              - device (int): Optional; default -1 (CPU).\n",
        "              - max_length (int): Optional; default 150.\n",
        "            For GPTAPIPipeline:\n",
        "              - model (str): Optional; default \"gpt-3.5-turbo\".\n",
        "              - temperature (float): Optional; default 0.0.\n",
        "              - max_tokens (int): Optional; default 150.\n",
        "        :return: An instance of ModelPipelineBase.\n",
        "        \"\"\"\n",
        "        source = source.lower()\n",
        "        if source in {\"huggingface\", \"local\"}:\n",
        "            if \"model_name\" not in kwargs:\n",
        "                raise ValueError(\"model_name must be provided for HuggingFace pipeline.\")\n",
        "            return HuggingFaceDetailedPipeline(\n",
        "                model_name=kwargs[\"model_name\"],\n",
        "                cache_dir=kwargs.get(\"cache_dir\"),\n",
        "                torch_dtype=kwargs.get(\"torch_dtype\", torch.float16),\n",
        "                temperature=kwargs.get(\"temperature\", 0.0),\n",
        "                device=kwargs.get(\"device\", -1),\n",
        "                max_length=kwargs.get(\"max_length\", 150)\n",
        "            )\n",
        "        elif source in {\"api\", \"gpt\"}:\n",
        "            return GPTAPIPipeline(\n",
        "                model=kwargs.get(\"model\", \"gpt-3.5-turbo\"),\n",
        "                temperature=kwargs.get(\"temperature\", 0.0),\n",
        "                max_tokens=kwargs.get(\"max_tokens\", 150)\n",
        "            )\n",
        "        elif source == \"mistral\":\n",
        "            return MistralPipeline(\n",
        "                api_key=kwargs.get(\"api_key\"),\n",
        "                model=kwargs.get(\"model\", \"mistral-7b\"),\n",
        "                temperature=kwargs.get(\"temperature\", 0.0),\n",
        "            )\n",
        "        elif source in {\"anthropic\", \"claude_sonnet\", \"sonnet\"}:\n",
        "            return ClaudeSonnetPipeline(\n",
        "                model=kwargs.get(\"model\", \"claude-3-7-sonnet-20250219\"),\n",
        "                temperature=kwargs.get(\"temperature\", 0.0),\n",
        "                max_tokens=kwargs.get(\"max_tokens\", 1024),\n",
        "            )\n",
        "        elif source in {\"llmapi\", \"llama_api\"}:\n",
        "            return LLMAPIPipeline(\n",
        "                model        = kwargs.get(\"model\", \"llama3.1-70b\"),\n",
        "                temperature  = kwargs.get(\"temperature\", 0.0),\n",
        "                max_tokens   = kwargs.get(\"max_tokens\", 256),\n",
        "                top_p        = kwargs.get(\"top_p\", None),\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported source: {source}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5dCmqrUiaQV"
      },
      "outputs": [],
      "source": [
        "def assemble_initial_prompt(transcript_dict: dict, with_context: bool, agent: str) -> str:\n",
        "    \"\"\"\n",
        "    Assemble a prompt instructing the agent to generate its INITIAL answer.\n",
        "\n",
        "    The prompt includes the question and, if requested, the context.\n",
        "    It then uses the appropriate template from DEBATER, replacing placeholders\n",
        "    for QUESTION, CONTEXT, AGENT, and leaves {ANSWER} as a marker.\n",
        "\n",
        "    Parameters:\n",
        "      transcript_dict (dict): Should contain at least \"question\" and optionally \"context\".\n",
        "      with_context (bool): If True, use the template including context.\n",
        "      agent (str): The agent identifier (e.g., \"A\" or \"B\").\n",
        "\n",
        "    Returns:\n",
        "      A formatted prompt string.\n",
        "    \"\"\"\n",
        "    question = transcript_dict.get(\"question\", \"\")\n",
        "\n",
        "    if with_context:\n",
        "        context = transcript_dict.get(\"context\", \"\")\n",
        "        template = DEBATER.get(\"initial_answer_prompt_with_context\", \"\")\n",
        "        prompt = template.format(QUESTION=question, CONTEXT=context)\n",
        "    else:\n",
        "        template = DEBATER.get(\"initial_answer_prompt_without_context\", \"\")\n",
        "        prompt = template.format(QUESTION=question)\n",
        "\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def assemble_transcript(\n",
        "    transcript_dict: dict,\n",
        "    num_rounds: int,\n",
        "    with_context: bool,\n",
        "    agent: str,\n",
        "    history_mode: str,   # \"full_history\" or \"last_round_history\"\n",
        "    debate_type: str,    # \"self\" or \"both\"\n",
        "    debate_mode: str,    # \"asymmetric\" or \"symmetric\"\n",
        "    role: str = None     # For asymmetric mode: \"challenger\" or \"defender\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Assemble a prompt for a debater.\n",
        "\n",
        "    Parameters:\n",
        "      transcript_dict: contains \"question\", \"context\", initial answers, and round responses.\n",
        "      num_rounds:      number of rounds completed so far.\n",
        "      with_context:    whether to include the context block.\n",
        "      agent:           \"A\" or \"B\".\n",
        "      history_mode:    \"full_history\" or \"last_round_history\".\n",
        "      debate_type:     \"self\" (only own initial) or \"both\" (both agents' initials).\n",
        "      debate_mode:     \"asymmetric\" or \"symmetric\".\n",
        "      role:            for asymmetric: \"challenger\" or \"defender\"; ignored if symmetric.\n",
        "\n",
        "    Returns:\n",
        "      A fully formatted prompt string for the agent.\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "\n",
        "    # 1) Pick the right instruction block based on symmetric/asymmetric + role\n",
        "    if debate_mode.lower() == \"asymmetric\":\n",
        "        if role is None:\n",
        "            instruction = \"Error: Asymmetric debate mode requires a role.\"\n",
        "        elif role.lower() == \"challenger\":\n",
        "            instruction = DEBATER[\"debater_ch_instru\"]\n",
        "        elif role.lower() == \"defender\":\n",
        "            instruction = DEBATER[\"debater_de_instru\"]\n",
        "        else:\n",
        "            instruction = \"Error: Unknown role provided.\"\n",
        "    elif debate_mode.lower() == \"symmetric\":\n",
        "        instruction = DEBATER[\"debater_sym_instru\"]\n",
        "    else:\n",
        "        instruction = \"Error: Unknown debate mode provided.\"\n",
        "\n",
        "    # 1.a) Prepend context‐reliability guidance\n",
        "    context_trust = DEBATER.get(\"context_trust_instru\", \"\")\n",
        "    instruction = f\"{context_trust}\\n\\n{instruction}\"\n",
        "    lines.append(\"Instruction:\")\n",
        "    lines.append(instruction)\n",
        "    lines.append(\"────────────────────────────\")\n",
        "\n",
        "    # 2) Question and optional context\n",
        "    question = transcript_dict.get(\"question\", \"\")\n",
        "    lines.append(f\"Question: {question}\")\n",
        "\n",
        "    if with_context:\n",
        "        context = transcript_dict.get(\"context\", \"\")\n",
        "        if context:\n",
        "            lines.append(f\"Context: {context}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # 3) If no rounds yet, ask for an initial answer\n",
        "    if num_rounds == 0:\n",
        "        lines.append(f\"Agent {agent}, please provide your INITIAL ANSWER based on the above information.\")\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "    # 4) Show initial answers\n",
        "    lines.append(\"Initial Answer(s):\")\n",
        "    if debate_type.lower() == \"self\":\n",
        "        key = f\"debater_{agent}_initial\"\n",
        "        lines.append(f\"  Agent {agent}: {transcript_dict.get(key,'')}\")\n",
        "    elif debate_type.lower() == \"both\":\n",
        "        lines.append(f\"  Agent A: {transcript_dict.get('debater_A_initial','')}\")\n",
        "        lines.append(f\"  Agent B: {transcript_dict.get('debater_B_initial','')}\")\n",
        "    else:\n",
        "        lines.append(\"Error: Unknown debate_type provided.\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # 5) Show previous round(s)\n",
        "    if history_mode.lower() == \"full_history\":\n",
        "        lines.append(\"Previous Round Responses (Full History):\")\n",
        "        for r in range(1, num_rounds):\n",
        "            lines.append(f\"  Round {r}:\")\n",
        "            lines.append(f\"    Agent A: {transcript_dict.get(f'debater_A_round_{r}','')}\")\n",
        "            lines.append(f\"    Agent B: {transcript_dict.get(f'debater_B_round_{r}','')}\")\n",
        "    elif history_mode.lower() == \"last_round_history\":\n",
        "        r = num_rounds\n",
        "        lines.append(\"Previous Round Responses (Last Round):\")\n",
        "        lines.append(f\"  Round {r}:\")\n",
        "        lines.append(f\"    Agent A: {transcript_dict.get(f'debater_A_round_{r}','')}\")\n",
        "        lines.append(f\"    Agent B: {transcript_dict.get(f'debater_B_round_{r}','')}\")\n",
        "    else:\n",
        "        lines.append(\"Error: Unknown history_mode provided.\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # 6) End‐of‐round thinking prompt\n",
        "    if num_rounds == 1:\n",
        "        end_instr = DEBATER[\"first_round_thinking\"]\n",
        "    elif num_rounds == 2:\n",
        "        end_instr = DEBATER[\"second_round_thinking\"]\n",
        "    else:\n",
        "        end_instr = DEBATER[\"nth_round_thinking\"]\n",
        "\n",
        "    lines.append(\"End Instruction:\")\n",
        "    lines.append(end_instr)\n",
        "    lines.append(\"\")\n",
        "    lines.append(f\"Agent {agent}, please provide your NEXT RESPONSE based on the above conversation.\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def assemble_judge_prompt(\n",
        "    transcript_dict: dict,\n",
        "    num_rounds: int,\n",
        "    with_context: bool,\n",
        "    judge_type: str = \"symmetric\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Assemble a prompt for the judge agent based on the debate transcript dictionary.\n",
        "\n",
        "    The prompt includes:\n",
        "      - A judge-specific instruction from JUDGE (selected based on judge_type),\n",
        "      - The complete debate transcript, including the question, (optionally) context, initial answers, and all round responses.\n",
        "      - A final instruction to generate the final decision, which is provided by the JUDGE template.\n",
        "\n",
        "    Parameters:\n",
        "      transcript_dict (dict): Contains \"question\", \"context\", initial answers, and round responses.\n",
        "      num_rounds (int): Number of completed debate rounds.\n",
        "      with_context (bool): Whether to include context.\n",
        "      judge_type (str): \"symmetric\" or \"asymmetric\" to select the appropriate judge instruction.\n",
        "\n",
        "    Returns:\n",
        "      A formatted prompt string for the judge.\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "\n",
        "    # Select judge instruction.\n",
        "    if judge_type.lower() == \"symmetric\":\n",
        "        judge_instruction = JUDGE.get(\"judge_sym_instru\", \"\")\n",
        "    elif judge_type.lower() == \"asymmetric\":\n",
        "        judge_instruction = JUDGE.get(\"judge_asym_instru\", \"\")\n",
        "    else:\n",
        "        judge_instruction = \"Error: Unknown judge type provided.\"\n",
        "\n",
        "    lines.append(\"Judge Instruction:\")\n",
        "    lines.append(judge_instruction)\n",
        "    lines.append(\"────────────────────────────\")\n",
        "\n",
        "    # Add the question.\n",
        "    question = transcript_dict.get(\"question\", \"\")\n",
        "    lines.append(\"Question: {QUESTION}\".format(QUESTION=question))\n",
        "\n",
        "    # Optionally, add context.\n",
        "    if with_context:\n",
        "        context = transcript_dict.get(\"context\", \"\")\n",
        "        if context:\n",
        "            lines.append(\"Context: {CONTEXT}\".format(CONTEXT=context))\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Add initial answers.\n",
        "    lines.append(\"Initial Answers:\")\n",
        "    lines.append(\"  Agent A: \" + transcript_dict.get(\"debater_A_initial\", \"\"))\n",
        "    lines.append(\"  Agent B: \" + transcript_dict.get(\"debater_B_initial\", \"\"))\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Add round responses (full history is preferred for the judge).\n",
        "    if num_rounds > 0:\n",
        "        lines.append(\"Round Responses:\")\n",
        "        for r in range(1, num_rounds):\n",
        "            lines.append(\"  Round {r}:\".format(r=r))\n",
        "            lines.append(\"    Agent A: \" + transcript_dict.get(f\"debater_A_round_{r}\", \"\"))\n",
        "            lines.append(\"    Agent B: \" + transcript_dict.get(f\"debater_B_round_{r}\", \"\"))\n",
        "    else:\n",
        "        lines.append(\"No debate rounds have been completed yet.\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Use the final decision prompt from JUDGE.\n",
        "    final_instr = JUDGE.get(\"final_decision_prompt\", \"\")\n",
        "    lines.append(final_instr)\n",
        "\n",
        "    return \"\\n\".join(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4vSXTVMhiPa"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Optional\n",
        "\n",
        "class Debater:\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str,\n",
        "        pipeline: ModelPipelineBase,\n",
        "        has_context: bool = False,\n",
        "        context: Optional[str] = None,\n",
        "        min_words: int = 5,\n",
        "        max_words: int = 50,\n",
        "        debate_mode: str = \"symmetric\",\n",
        "        debate_type: str = \"both\",\n",
        "        history_mode: str = \"full_history\",\n",
        "        role: Optional[str] = None,\n",
        "    ):\n",
        "        self.name          = name\n",
        "        self.pipeline      = pipeline\n",
        "        self.has_context   = has_context\n",
        "        self.context       = context\n",
        "        self.min_words     = min_words\n",
        "        self.max_words     = max_words\n",
        "        self.debate_mode   = debate_mode       # \"symmetric\" or \"asymmetric\"\n",
        "        self.debate_type = debate_type     # \"self\" or \"both\"\n",
        "        self.history_mode  = history_mode      # \"full_history\" or \"last_round_history\"\n",
        "        self.role          = role              # only used if asym\n",
        "\n",
        "    def get_initial_answer(self, transcript_dict: dict) -> str:\n",
        "        prompt  = assemble_initial_prompt(transcript_dict,\n",
        "                                         with_context=self.has_context,\n",
        "                                         agent=self.name)\n",
        "        response = self.pipeline.call_model(prompt)\n",
        "        # extract after \"answer:\"\n",
        "        #m = re.search(r\"answer:\\s*(.*)\", response, re.IGNORECASE)\n",
        "        #return m.group(1).strip() if m else response\n",
        "        return response\n",
        "\n",
        "    def debate_response(self, transcript_dict: dict, round_num: int) -> str:\n",
        "        prompt = assemble_transcript(\n",
        "            transcript_dict,\n",
        "            num_rounds=round_num,\n",
        "            with_context=self.has_context,\n",
        "            agent=self.name,\n",
        "            history_mode=self.history_mode,\n",
        "            debate_type=self.debate_type,   # “self” or “both”\n",
        "            debate_mode=self.debate_mode,   # “symmetric” or “asymmetric”\n",
        "            role=self.role,\n",
        "        )\n",
        "        return self.pipeline.call_model(prompt)\n",
        "\n",
        "    def extract_argument(self, response: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract the text within <argument>...</argument> tags.\n",
        "        \"\"\"\n",
        "        match = re.search(r\"<argument>(.*?)</argument>\", response)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "        return response.strip()\n",
        "\n",
        "    def truncate(self, argument: str) -> str:\n",
        "        \"\"\"\n",
        "        Truncate the argument to the maximum word count.\n",
        "        \"\"\"\n",
        "        words = argument.split()\n",
        "        if len(words) > self.max_words:\n",
        "            truncated = \" \".join(words[:self.max_words]) + \"... <TRUNCATED>\"\n",
        "            return truncated\n",
        "        return argument\n",
        "\n",
        "\n",
        "class Judge:\n",
        "    def __init__(self, name: str, pipeline: ModelPipelineBase):\n",
        "        \"\"\"\n",
        "        Initialize the Judge.\n",
        "        \"\"\"\n",
        "        self.name = name\n",
        "        self.pipeline = pipeline\n",
        "\n",
        "    def decide(self, transcript: str) -> str:\n",
        "        \"\"\"\n",
        "        Generate a final decision based on the debate transcript.\n",
        "        Uses the appropriate judge instruction from the JUDGE template.\n",
        "        \"\"\"\n",
        "        prompt = JUDGE.get(\"judge_sym_instru\", \"\").format(name=self.name, transcript=transcript)\n",
        "        response = self.pipeline.call_model(prompt)\n",
        "        decision = self.extract_clean_argument(response)\n",
        "        return f\"[{self.name}] Final Decision: {decision}\"\n",
        "\n",
        "    def extract_clean_argument(self, response: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract the text within <argument>...</argument> tags from the judge's response.\n",
        "        \"\"\"\n",
        "        match = re.search(r\"<argument>(.*?)</argument>\", response)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "        return response.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vya-XnlTZzx_"
      },
      "source": [
        "### Basic Debate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr1orK7_EWYq"
      },
      "source": [
        "#### llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JK9HAQF9EcWq"
      },
      "outputs": [],
      "source": [
        "# 1) Load the 600-item mixed dataset (300 standard + 300 perturbed)\n",
        "with open(\"sc_mad_mixed_600.json\") as f:\n",
        "    tasks = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bD_zMDoaEYR9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "# 1) Instantiate pipelines & agents once\n",
        "batch_size = 150\n",
        "\n",
        "pipelineA = ModelPipelineFactory.create_pipeline(\n",
        "    source=\"llmapi\",                 # ← was \"api\"\n",
        "    model=\"llama3.3-70b\",           # any model name LLM API hosts\n",
        "    temperature=0.0,\n",
        "    max_tokens=200,\n",
        ")\n",
        "\n",
        "pipelineB = ModelPipelineFactory.create_pipeline(\n",
        "    source=\"llmapi\",\n",
        "    model=\"llama3.3-70b\",\n",
        "    temperature=0.0,\n",
        "    max_tokens=200,\n",
        ")\n",
        "\n",
        "pipelineJ = ModelPipelineFactory.create_pipeline(\n",
        "    source=\"llmapi\",\n",
        "    model=\"llama3.3-70b\",\n",
        "    temperature=0.0,\n",
        "    max_tokens=150,\n",
        ")\n",
        "\n",
        "# pipelineA = ModelPipelineFactory.create_pipeline(\n",
        "#     source=\"llmapi\",                 # ← was \"api\"\n",
        "#     model=\"llama3.1-8b\",           # any model name LLM API hosts\n",
        "#     temperature=0.0,\n",
        "#     max_tokens=200,\n",
        "# )\n",
        "\n",
        "# pipelineB = ModelPipelineFactory.create_pipeline(\n",
        "#     source=\"llmapi\",\n",
        "#     model=\"llama3.1-8b\",\n",
        "#     temperature=0.0,\n",
        "#     max_tokens=200,\n",
        "# )\n",
        "\n",
        "# pipelineJ = ModelPipelineFactory.create_pipeline(\n",
        "#     source=\"llmapi\",\n",
        "#     model=\"llama3.1-8b\",\n",
        "#     temperature=0.0,\n",
        "#     max_tokens=50,\n",
        "# )\n",
        "\n",
        "debaterA = Debater(\"A\", pipelineA, has_context=True, debate_mode=\"symmetric\", debate_type=\"both\", history_mode=\"full_history\")\n",
        "debaterB = Debater(\"B\", pipelineB, has_context=False, debate_mode=\"symmetric\", debate_type=\"both\", history_mode=\"full_history\")\n",
        "judge     = Judge(  \"Judge\", pipelineJ)\n",
        "\n",
        "\n",
        "# tasks = []\n",
        "# for item in qa_dataset:\n",
        "#     base_q   = item[\"Question\"]\n",
        "#     std_ctx  = item.get(\"Context\", None)            # original context\n",
        "#     std_ans  = item[\"StandardAnswer\"]               # ground‐truth\n",
        "\n",
        "#     # 1) standard ICL task (no perturbation)\n",
        "#     tasks.append({\n",
        "#         \"question\":         base_q,\n",
        "#         \"context\":          std_ctx,\n",
        "#         \"standard_answer\":  std_ans,\n",
        "#         \"offset\":           \"0\",\n",
        "#         \"perturbed_answer\": std_ans,                 # same as standard\n",
        "#                # you can check this downstream\n",
        "#     })\n",
        "\n",
        "# 3) Stage 1: batch‐call initial answers for both agents\n",
        "prompts_A0 = [assemble_initial_prompt(t, debaterA.has_context,  debaterA.name) for t in tasks]\n",
        "prompts_B0 = [assemble_initial_prompt(t, debaterB.has_context,  debaterB.name) for t in tasks]\n",
        "\n",
        "answers_A0 = pipelineA.call_model_batch(prompts_A0, batch_size)\n",
        "answers_B0 = pipelineB.call_model_batch(prompts_B0, batch_size)\n",
        "\n",
        "for i,t in enumerate(tasks):\n",
        "    t[\"debater_A_initial\"] = answers_A0[i]\n",
        "    t[\"debater_B_initial\"] = answers_B0[i]\n",
        "\n",
        "# 4) Stage 2: for each round, batch both agents’ responses\n",
        "rounds = 5\n",
        "for r in range(5, rounds+1):\n",
        "    # Agent A\n",
        "    prompts_Ar = [\n",
        "        assemble_transcript(t, num_rounds=r, with_context=debaterA.has_context,\n",
        "                            agent=debaterA.name, history_mode=debaterA.history_mode,\n",
        "                            debate_mode=debaterA.debate_mode, debate_type=debaterA.debate_type,\n",
        "                            role=None)\n",
        "        for t in tasks\n",
        "    ]\n",
        "    resp_Ar = pipelineA.call_model_batch(prompts_Ar, batch_size)\n",
        "    for i,t in enumerate(tasks):\n",
        "        t[f\"debater_A_round_{r}\"] = resp_Ar[i]\n",
        "\n",
        "    # Agent B\n",
        "    prompts_Br = [\n",
        "        assemble_transcript(t, num_rounds=r, with_context=debaterB.has_context,\n",
        "                            agent=debaterB.name, history_mode=debaterB.history_mode,\n",
        "                            debate_mode=debaterB.debate_mode, debate_type=debaterB.debate_type,\n",
        "                            role=None)\n",
        "        for t in tasks\n",
        "    ]\n",
        "    resp_Br = pipelineB.call_model_batch(prompts_Br, batch_size)\n",
        "    for i,t in enumerate(tasks):\n",
        "        t[f\"debater_B_round_{r}\"] = resp_Br[i]\n",
        "\n",
        "    # 5) Stage 3: batch‐call judge decisions\n",
        "    prompts_J = [\n",
        "        assemble_judge_prompt(t, num_rounds=r, with_context=False, judge_type=\"symmetric\")\n",
        "        for t in tasks\n",
        "    ]\n",
        "    outs_J = pipelineJ.call_model_batch(prompts_J, batch_size)\n",
        "    for i,t in enumerate(tasks):\n",
        "        t[f\"raw_judge_output_{r}\"] = outs_J[i]\n",
        "        m = re.search(r\"final answer:\\s*(\\d{4})\", outs_J[i], re.IGNORECASE)\n",
        "        t[f\"final_year_{r}\"] = m.group(1) if m else None\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rounds = 5\n",
        "for r in range(5, rounds+1):\n",
        "    # Agent A\n",
        "    prompts_Ar = [\n",
        "        assemble_transcript(t, num_rounds=r, with_context=debaterA.has_context,\n",
        "                            agent=debaterA.name, history_mode=debaterA.history_mode,\n",
        "                            debate_mode=debaterA.debate_mode, debate_type=debaterA.debate_type,\n",
        "                            role=None)\n",
        "        for t in tasks\n",
        "    ]\n",
        "    resp_Ar = pipelineA.call_model_batch(prompts_Ar, batch_size)\n",
        "    for i,t in enumerate(tasks):\n",
        "        t[f\"debater_A_round_{r}\"] = resp_Ar[i]\n",
        "\n",
        "    # Agent B\n",
        "    prompts_Br = [\n",
        "        assemble_transcript(t, num_rounds=r, with_context=debaterB.has_context,\n",
        "                            agent=debaterB.name, history_mode=debaterB.history_mode,\n",
        "                            debate_mode=debaterB.debate_mode, debate_type=debaterB.debate_type,\n",
        "                            role=None)\n",
        "        for t in tasks\n",
        "    ]\n",
        "    resp_Br = pipelineB.call_model_batch(prompts_Br, batch_size)\n",
        "    for i,t in enumerate(tasks):\n",
        "        t[f\"debater_B_round_{r}\"] = resp_Br[i]\n",
        "\n",
        "    # 5) Stage 3: batch‐call judge decisions\n",
        "    prompts_J = [\n",
        "        assemble_judge_prompt(t, num_rounds=r, with_context=False, judge_type=\"symmetric\")\n",
        "        for t in tasks\n",
        "    ]\n",
        "    outs_J = pipelineJ.call_model_batch(prompts_J, batch_size)\n",
        "    for i,t in enumerate(tasks):\n",
        "        t[f\"raw_judge_output_{r}\"] = outs_J[i]\n",
        "        m = re.search(r\"final answer:\\s*(\\d{4})\", outs_J[i], re.IGNORECASE)\n",
        "        t[f\"final_year_{r}\"] = m.group(1) if m else None\n"
      ],
      "metadata": {
        "id": "DPKtj6BiVPnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 150\n",
        "pipelineJ = ModelPipelineFactory.create_pipeline(\n",
        "    source=\"llmapi\",\n",
        "    model=\"llama3.1-70b\",\n",
        "    temperature=0.0,\n",
        "    max_tokens=250,\n",
        ")\n",
        "\n",
        "rounds = 5\n",
        "for r in range(1, rounds):\n",
        "    prompts_J = [\n",
        "        assemble_judge_prompt(t, num_rounds=r, with_context=False, judge_type=\"symmetric\")\n",
        "        for t in tasks\n",
        "    ]\n",
        "    outs_J = pipelineJ.call_model_batch(prompts_J, batch_size)\n",
        "    for i,t in enumerate(tasks):\n",
        "        t[f\"raw_judge_output_{r}\"] = outs_J[i]\n",
        "        m = re.search(r\"final answer:\\s*(\\d{4})\", outs_J[i], re.IGNORECASE)\n",
        "        t[f\"final_year_{r}\"] = m.group(1) if m else None"
      ],
      "metadata": {
        "id": "dnMiC6TFuqoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG5K2whLZ2SZ"
      },
      "source": [
        "### Comparing Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NwtfSGsblR5"
      },
      "source": [
        "#### llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ly2w5iKscpqN",
        "outputId": "9f433dad-9829-41d1-c48e-55a27ae5de84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2007\n"
          ]
        }
      ],
      "source": [
        "pipe = ModelPipelineFactory.create_pipeline(\"llmapi\")\n",
        "print(pipe.call_model(\"What year was the first iPhone released? Answer only with a 4-digit year.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rY5NHzajcpqO",
        "outputId": "c272e498-5188-4ec8-8d9b-fcbb09b5c155"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Few-shot w/ context  –  Standard:  100.000%  (300/300)\n",
            "Few-shot w/ context  –  Perturbed: 10.000%  (30/300)\n",
            "Few-shot w/ context  –  Overall:   55.000%  (330/600)\n"
          ]
        }
      ],
      "source": [
        "import os, random, json, re, math\n",
        "#from pipeline import ModelPipelineFactory\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 0.  Load the 600-item mixed dataset\n",
        "# --------------------------------------------------\n",
        "with open(\"sc_mad_mixed_600.json\") as f:\n",
        "    tasks = json.load(f)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 1.  Build a few-shot header *with context*\n",
        "# --------------------------------------------------\n",
        "FEWSHOT_EX_N = 4\n",
        "fewshot_ex = random.sample([t for t in tasks if t[\"context\"]], FEWSHOT_EX_N)\n",
        "\n",
        "example_blocks = []\n",
        "for ex in fewshot_ex:\n",
        "    example_blocks.append(\n",
        "        f\"Context: {ex['context']}\\n\"\n",
        "        f\"Question: {ex['question']}\\n\"\n",
        "        f\"Answer: {ex['standard_answer']}\"\n",
        "    )\n",
        "\n",
        "FEWSHOT_HEADER = (\n",
        "    \"You are a QA bot. Answer ONLY with the 4-digit year. \"\n",
        "    \"If unsure, give your best guess.\\n\\n\"\n",
        "    + \"\\n\\n\".join(example_blocks)\n",
        "    + \"\\n\\n\"\n",
        ")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 2.  Build one prompt per item  (context + question)\n",
        "# --------------------------------------------------\n",
        "prompts = [\n",
        "    FEWSHOT_HEADER\n",
        "    + f\"Context: {t['context']}\\n\"\n",
        "    + f\"Question: {t['question']}\\n\"\n",
        "    + \"Answer:\"\n",
        "    for t in tasks\n",
        "]\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 3.  Run the model in batches via Mistral 7B\n",
        "# --------------------------------------------------\n",
        "pipelineFS = ModelPipelineFactory.create_pipeline(\n",
        "    source=\"llmapi\",                # was \"api\"\n",
        "    model= \"llama3.3-70b\", #\"llama3.1-8b\", #\"claude-3-7-sonnet-20250219\",         # was \"gpt-3.5-turbo\"\n",
        "    temperature=0.0,\n",
        "    max_tokens=50,                     # Sonnet can stream up to 2000 anyway\n",
        ")\n",
        "BATCH_SIZE = 150\n",
        "responses = pipelineFS.call_model_batch(prompts, batch_size=BATCH_SIZE)  # inherits your batching logic :contentReference[oaicite:0]{index=0}\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 4.  Parse answers & score\n",
        "# --------------------------------------------------\n",
        "YEAR_RE = re.compile(r\"\\b(\\d{4})\\b\")\n",
        "\n",
        "for t, resp in zip(tasks, responses):\n",
        "    m = YEAR_RE.search(resp)\n",
        "    yr = m.group(1) if m else None\n",
        "    t[\"fewshot_ctx_raw\"]     = resp.strip()\n",
        "    t[\"fewshot_ctx_year\"]    = yr\n",
        "    t[\"fewshot_ctx_correct\"] = (yr == t[\"standard_answer\"])\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 5.  Quick metrics\n",
        "# --------------------------------------------------\n",
        "std_ok = sum(t[\"fewshot_ctx_correct\"] for t in tasks if t[\"offset\"] == \"0\")\n",
        "per_ok = sum(t[\"fewshot_ctx_correct\"] for t in tasks if t[\"offset\"] != \"0\")\n",
        "std_tot = sum(t[\"offset\"] == \"0\" for t in tasks)\n",
        "per_tot = sum(t[\"offset\"] != \"0\" for t in tasks)\n",
        "\n",
        "print(f\"Few-shot w/ context  –  Standard:  {std_ok/std_tot:.3%}  \"\n",
        "      f\"({std_ok}/{std_tot})\")\n",
        "print(f\"Few-shot w/ context  –  Perturbed: {per_ok/per_tot:.3%}  \"\n",
        "      f\"({per_ok}/{per_tot})\")\n",
        "print(f\"Few-shot w/ context  –  Overall:   \"\n",
        "      f\"{(std_ok+per_ok)/len(tasks):.3%}  ({std_ok+per_ok}/{len(tasks)})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c86ysoz9cpqO"
      },
      "outputs": [],
      "source": [
        "\"\"\"### self-ask\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# 2) Build the Self-Ask pipeline on Mistral 7B\n",
        "pipe_sa = ModelPipelineFactory.create_pipeline(\n",
        "    source=\"llmapi\",                # was \"api\"\n",
        "    model= \"llama3.1-405b\", #\"llama3.1-8b\", #\"claude-3-7-sonnet-20250219\",         # was \"gpt-3.5-turbo\"\n",
        "    temperature=0.0,\n",
        "    max_tokens=120,                     # Sonnet can stream up to 2000 anyway\n",
        ")\n",
        "\n",
        "# 3) Build prompts\n",
        "prompts_sa = [\n",
        "    SELF_ASK_PROMPT.format(\n",
        "        CONTEXT=t[\"context\"] or \"\",\n",
        "        QUESTION=t[\"question\"]\n",
        "    )\n",
        "    for t in tasks\n",
        "]\n",
        "\n",
        "# 4) Batch-call the model\n",
        "batch_size = 150\n",
        "self_ask_outs = pipe_sa.call_model_batch(prompts_sa, batch_size)\n",
        "\n",
        "# 5) Parse out “Final answer: YYYY” and compare to ground truth\n",
        "YEAR_RE = re.compile(r\"Final answer:\\s*(\\d{4})\", re.IGNORECASE)\n",
        "\n",
        "for t, out in zip(tasks, self_ask_outs):\n",
        "    m = YEAR_RE.search(out)\n",
        "    year = m.group(1) if m else None\n",
        "    t[\"selfask_pred\"]    = year\n",
        "    t[\"selfask_raw\"]     = out.strip()\n",
        "    t[\"selfask_correct\"] = (year == t[\"standard_answer\"])\n",
        "\n",
        "# 6) Compute accuracy splits\n",
        "std = [x for x in tasks if x[\"offset\"] == \"0\"]\n",
        "prb = [x for x in tasks if x[\"offset\"] != \"0\"]\n",
        "\n",
        "print(\"Self-Ask EM accuracy (Mistral 7B):\")\n",
        "print(f\" • Standard : {sum(x['selfask_correct'] for x in std)/len(std):.1%}\")\n",
        "print(f\" • Perturbed: {sum(x['selfask_correct'] for x in prb)/len(prb):.1%}\")\n",
        "print(f\" • Overall  : {sum(x['selfask_correct'] for x in tasks)/len(tasks):.1%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tr1WjNxAcpqa"
      },
      "outputs": [],
      "source": [
        "# %% Cell 5 – RCI Step 1: Initial Answers\n",
        "prompts_init = [\n",
        "    DEBATER[\"rci_initial_prompt\"].format(\n",
        "        CONTEXT=t[\"context\"] or \"\",\n",
        "        QUESTION=t[\"question\"]\n",
        "    )\n",
        "    for t in tasks\n",
        "]\n",
        "outs_init = pipe.call_model_batch(prompts_init, batch_size=BATCH_SIZE)\n",
        "\n",
        "# store drafts\n",
        "for t, draft in zip(tasks, outs_init):\n",
        "    # extract 4-digit if present\n",
        "    m = re.search(r\"\\b(\\d{4})\\b\", draft)\n",
        "    t[\"rci_draft\"] = draft.strip()\n",
        "    t[\"rci_draft_year\"] = m.group(1) if m else None\n",
        "\n",
        "# %% Cell 6 – RCI Step 2: Critique\n",
        "prompts_crit = [\n",
        "    DEBATER[\"rci_critique_prompt\"].format(DRAFT=t[\"rci_draft\"], CONTEXT=t[\"context\"], QUESTION=t[\"question\"])\n",
        "    for t in tasks\n",
        "]\n",
        "outs_crit = pipe.call_model_batch(prompts_crit, batch_size=BATCH_SIZE)\n",
        "\n",
        "for t, crit in zip(tasks, outs_crit):\n",
        "    t[\"rci_critique\"] = crit.strip()\n",
        "\n",
        "# %% Cell 7 – RCI Step 3: Revision\n",
        "prompts_rev = [\n",
        "    DEBATER[\"rci_revise_prompt\"].format(\n",
        "        DRAFT=t[\"rci_draft\"],\n",
        "        CRITIQUE=t[\"rci_critique\"],\n",
        "        CONTEXT=t[\"context\"],\n",
        "        QUESTION=t[\"question\"],\n",
        "    )\n",
        "    for t in tasks\n",
        "]\n",
        "outs_rev = pipe.call_model_batch(prompts_rev, batch_size=BATCH_SIZE)\n",
        "\n",
        "for t, final in zip(tasks, outs_rev):\n",
        "    m = re.search(r\"\\b(\\d{4})\\b\", final)\n",
        "    t[\"rci_final\"] = final.strip()\n",
        "    t[\"rci_final_year\"] = m.group(1) if m else None\n",
        "    # exact-match correctness\n",
        "    t[\"rci_correct\"] = (t[\"rci_final_year\"] == t[\"standard_answer\"])\n",
        "\n",
        "# %% Cell 8 – Accuracy Summary\n",
        "std  = [t for t in tasks if t[\"offset\"] == \"0\"]\n",
        "pert = [t for t in tasks if t[\"offset\"] != \"0\"]\n",
        "\n",
        "def acc(key, subset):\n",
        "    return 100*sum(1 for t in subset if t[key])/len(subset)\n",
        "\n",
        "print(\"RCI Exact-Match Accuracy:\")\n",
        "print(f\" • Standard : {acc('rci_correct', std):.1f}% ({len(std)} items)\")\n",
        "print(f\" • Perturbed: {acc('rci_correct', pert):.1f}% ({len(pert)} items)\")\n",
        "print(f\" • Overall  : {acc('rci_correct', tasks):.1f}% ({len(tasks)} items)\")\n",
        "\n",
        "# 14 min"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4OPypTalaU1"
      },
      "source": [
        "### Naive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3W20ovClaB3"
      },
      "outputs": [],
      "source": [
        "# ── 2.  Prompt assembly helpers ─────────────────────────────────────────────\n",
        "def make_initial_prompt(task: dict, agent_idx: int) -> str:\n",
        "    return INITIAL_PROMPT.format(\n",
        "        AGENT_ID=agent_idx,\n",
        "        QUESTION=task[\"question\"],\n",
        "        CONTEXT=task[\"context\"]\n",
        "    )\n",
        "\n",
        "def make_round_prompt(task: dict,\n",
        "                      agent_idx: int,\n",
        "                      round_idx: int,\n",
        "                      num_agents: int) -> str:\n",
        "    # collect peers’ last answers (excluding self)\n",
        "    peers = []\n",
        "    for j in range(num_agents):\n",
        "        if j == agent_idx:\n",
        "            continue\n",
        "        key = \"agent_{j}_{r}\".format(j=j, r=round_idx-1)\n",
        "        peers.append(f\"Agent {j}: {task[key]}\")\n",
        "    peer_block = \"\\n\".join(peers)\n",
        "\n",
        "    own_key = \"agent_{i}_{r}\".format(i=agent_idx, r=round_idx-1)\n",
        "    own_ans = task[own_key]\n",
        "\n",
        "    return CONSENSUS_PROMPT.format(\n",
        "        AGENT_ID=agent_idx,\n",
        "        PEER_ANSWERS=peer_block,\n",
        "        OWN_ANSWER=own_ans,\n",
        "        QUESTION=task[\"question\"],\n",
        "        CONTEXT=task[\"context\"]\n",
        "    )\n",
        "# ── 4.  Tiny helper to grab ▢ 1859 ▢ → \"1859\" ───────────────────────────────\n",
        "def extract_boxed(text: str) -> str:\n",
        "    m = re.search(r\"▢\\s*([\\s\\S]*?)\\s*▢\", text)\n",
        "    return m.group(1).strip() if m else text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFGV9sjyqZ_T"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1.  Simple helper that hides pipeline.call_model_batch\n",
        "# ------------------------------------------------------------\n",
        "def run_llmapi_batch(prompts: list[str],\n",
        "                     pipeline: ModelPipelineBase,\n",
        "                     batch_size: int = 150) -> list[str]:\n",
        "    \"\"\"\n",
        "    Thin wrapper so the rest of the code looks like the old\n",
        "    `run_anthropic_chunks`.  Returns a *flat* list of responses\n",
        "    in the same order as `prompts`.\n",
        "    \"\"\"\n",
        "    return pipeline.call_model_batch(prompts, batch_size=batch_size)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2.  The Naive debate driver (LLMAPI edition)\n",
        "# ------------------------------------------------------------\n",
        "def run_naive_debate_llmapi(tasks: list[dict],\n",
        "                            pipeline: ModelPipelineBase,\n",
        "                            num_agents: int  = 3,\n",
        "                            max_rounds: int  = 3,\n",
        "                            batch_size: int  = 150,\n",
        "                            unanimity_break: bool = True) -> None:\n",
        "    \"\"\"\n",
        "    Mutates each element of `tasks` in-place, adding:\n",
        "        • agent_{i}_{r}  – every agent’s answer per round\n",
        "        • group_answer   – final answer after convergence / majority vote\n",
        "    \"\"\"\n",
        "\n",
        "    # ---------- Round 0 : independent answers ----------\n",
        "    init_prompts, mapping = [], []          # mapping → (task_idx, agent_idx)\n",
        "    for t_idx, task in enumerate(tasks):\n",
        "        for i in range(num_agents):\n",
        "            init_prompts.append(make_initial_prompt(task, i))\n",
        "            mapping.append((t_idx, i))\n",
        "\n",
        "    init_responses = run_llmapi_batch(init_prompts, pipeline, batch_size)\n",
        "    for (t_idx, i), txt in zip(mapping, init_responses):\n",
        "        tasks[t_idx][f\"agent_{i}_0\"] = txt\n",
        "\n",
        "    # ---------- Subsequent consensus rounds ----------\n",
        "    for r in range(1, max_rounds + 1):\n",
        "        round_prompts, mapping = [], []\n",
        "        for t_idx, task in enumerate(tasks):\n",
        "            for i in range(num_agents):\n",
        "                round_prompts.append(\n",
        "                    make_round_prompt(task, i, r, num_agents)\n",
        "                )\n",
        "                mapping.append((t_idx, i))\n",
        "\n",
        "        round_responses = run_llmapi_batch(round_prompts, pipeline, batch_size)\n",
        "        for (t_idx, i), txt in zip(mapping, round_responses):\n",
        "            tasks[t_idx][f\"agent_{i}_{r}\"] = txt\n",
        "\n",
        "        # ---------- Early stopping on unanimity ----------\n",
        "        if unanimity_break:\n",
        "            remaining = []\n",
        "            for task in tasks:\n",
        "                latest = [task[f\"agent_{i}_{r}\"] for i in range(num_agents)]\n",
        "                yrs    = {extract_boxed(ans) for ans in latest}\n",
        "                if len(yrs) == 1:\n",
        "                    task[\"group_answer\"] = yrs.pop()\n",
        "                else:\n",
        "                    remaining.append(task)\n",
        "            if not remaining:\n",
        "                print(f\"✓ All tasks unanimous after round {r}\")\n",
        "                break\n",
        "            tasks = remaining\n",
        "        print(f\"✓ Finished round {r}\")\n",
        "\n",
        "    # ---------- Majority vote for anything still unresolved ----------\n",
        "    for task in tasks:\n",
        "        if \"group_answer\" in task:\n",
        "            continue\n",
        "        final_r = min(r, max_rounds)\n",
        "        final_answers = [\n",
        "            extract_boxed(task[f\"agent_{i}_{final_r}\"])\n",
        "            for i in range(num_agents)\n",
        "        ]\n",
        "        task[\"group_answer\"] = max(set(final_answers), key=final_answers.count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYHOshG-nXwg"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 0.  Build *one* LLMAPI pipeline that every agent will share\n",
        "# ============================================================\n",
        "NAIVE_MODEL   = \"llama3.3-70b\"          # or any other model LLMAPI hosts\n",
        "NAIVE_TEMP    = 0.7                     # matches the old Anthropic setting\n",
        "NAIVE_MAXTOK  = 200\n",
        "NAIVE_BSIZE   = 150                     # how many prompts per API call\n",
        "\n",
        "llmapi_pipe = ModelPipelineFactory.create_pipeline(\n",
        "    source      = \"llmapi\",\n",
        "    model       = NAIVE_MODEL,\n",
        "    temperature = NAIVE_TEMP,\n",
        "    max_tokens  = NAIVE_MAXTOK,\n",
        ")\n",
        "\n",
        "DATASET_SIZE   = len(tasks)\n",
        "NUM_AGENTS     = 3\n",
        "ROUNDS         = 5\n",
        "\n",
        "for t in tasks:\n",
        "    t[\"debate_model_id\"] = NAIVE_MODEL\n",
        "    t[\"debate class\"]    = \"Naive-LLMAPI\"\n",
        "\n",
        "run_naive_debate_llmapi(\n",
        "    tasks,\n",
        "    pipeline      = llmapi_pipe,\n",
        "    num_agents    = NUM_AGENTS,\n",
        "    max_rounds    = ROUNDS,\n",
        "    batch_size    = NAIVE_BSIZE,\n",
        "    unanimity_break = True\n",
        ")\n",
        "\n",
        "# Save the results\n",
        "ts   = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "ofile = f\"ND_R{ROUNDS}_A{NUM_AGENTS}_QA{DATASET_SIZE}_{NAIVE_MODEL}_{ts}.json\"\n",
        "with open(ofile, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(tasks, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"All done →\", ofile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4akSuFBlcP_"
      },
      "source": [
        "### Rea"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wcyULV7SldM2"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import math\n",
        "# 1) Instantiate pipelines & agents once\n",
        "batch_size = 150\n",
        "SC_THRESHOLD = 0.9\n",
        "rounds = 5\n",
        "\n",
        "pipelineD = ModelPipelineFactory.create_pipeline(source=\"llmapi\",  model=\"llama3.3-70b\", temperature=0.0, max_tokens=250)\n",
        "pipelineC = ModelPipelineFactory.create_pipeline(source=\"llmapi\",  model=\"llama3.3-70b\", temperature=0.0, max_tokens=250)\n",
        "pipelineJ = ModelPipelineFactory.create_pipeline(source=\"llmapi\",  model=\"llama3.3-70b\", temperature=0.0, max_tokens=50)\n",
        "\n",
        "\n",
        "\n",
        "def_prompts = [\n",
        "    R_DEBATER[\"defender_initial\"].format(CONTEXT=t[\"context\"], QUESTION=t[\"question\"])\n",
        "    for t in tasks\n",
        "]\n",
        "def_resps = pipelineD.call_model_batch(def_prompts, batch_size)\n",
        "\n",
        "crit_prompts = [\n",
        "    R_DEBATER[\"decider_initial\"].format(CONTEXT=t[\"context\"], QUESTION=t[\"question\"])\n",
        "    for t in tasks\n",
        "]\n",
        "crit_resps = pipelineC.call_model_batch(crit_prompts, batch_size)\n",
        "\n",
        "arg_re = re.compile(r\"<argument>(.*?)</argument>\", re.DOTALL)\n",
        "for t, d_resp, c_resp in zip(tasks, def_resps, crit_resps):\n",
        "    t[\"defender_argument_0\"] = (arg_re.search(d_resp) or [None, d_resp])[1].strip()\n",
        "    t[\"critic_argument_0\"]   = (arg_re.search(c_resp) or [None, c_resp])[1].strip()\n",
        "\n",
        "for r in range(1, rounds + 1):\n",
        "    defender_prompts = []\n",
        "    critic_prompts   = []\n",
        "\n",
        "    for t in tasks:\n",
        "        # 3.a) Build the transcript of rounds 0…r-1\n",
        "        rounds_text = \"\"\n",
        "        for i in range(0, r):  # include opening (i=0) up to round r-1\n",
        "            rounds_text += R_Transcript[\"single_round_template\"].format(\n",
        "                round_number=i,\n",
        "                defender_argument=t[f\"defender_argument_{i}\"],\n",
        "                critic_argument=t[f\"critic_argument_{i}\"]\n",
        "            )\n",
        "        full_history = R_Transcript[\"debate_transcript_template\"].format(rounds=rounds_text)\n",
        "\n",
        "        defender_prompts.append(\n",
        "            R_DEBATER[\"defender_followup_concise\"].format(\n",
        "                full_history=full_history,\n",
        "                name=\"Defender\",\n",
        "            )\n",
        "        )\n",
        "        critic_prompts.append(\n",
        "            R_DEBATER[\"decider_followup\"].format(\n",
        "                full_history=full_history,\n",
        "                name=\"Decider\",\n",
        "            )\n",
        "        )\n",
        "    # 3.c) Batch-call your pipelines\n",
        "    new_defs  = pipelineD.call_model_batch(defender_prompts, batch_size)\n",
        "    new_crits = pipelineC.call_model_batch(critic_prompts,  batch_size)\n",
        "    # 3.d) Extract and store arguments\n",
        "    for t, d_resp, c_resp in zip(tasks, new_defs, new_crits):\n",
        "        t[f\"defender_argument_{r}\"] = (arg_re.search(d_resp) or [None, d_resp])[1].strip()\n",
        "        t[f\"critic_argument_{r}\"]   = (arg_re.search(c_resp) or [None, c_resp])[1].strip()\n",
        "\n",
        "        # 3.b) Judge for this round r\n",
        "    judge_prompts = []\n",
        "    for t in tasks:\n",
        "        rounds_text = \"\"\n",
        "        for i in range(0, r + 1):\n",
        "            rounds_text += R_Transcript[\"single_round_template\"].format(\n",
        "                round_number=i,\n",
        "                defender_argument=t[f\"defender_argument_{i}\"],\n",
        "                critic_argument=t[f\"critic_argument_{i}\"]\n",
        "            )\n",
        "        full_history = R_Transcript[\"debate_transcript_template\"].format(rounds=rounds_text)\n",
        "        judge_prompts.append(\n",
        "            R_JUDGE[\"judge_instruction\"].format(full_history=full_history)\n",
        "        )\n",
        "\n",
        "    judge_outs = pipelineJ.call_model_batch(judge_prompts, batch_size)\n",
        "    for t, jout in zip(tasks, judge_outs):\n",
        "        t[f\"raw_judge_output_round_{r}\"] = jout\n",
        "        m = re.search(r\"final decision:\\s*(REASONABLE|UNREASONABLE)\", jout, re.IGNORECASE)\n",
        "        t[f\"context_reasonable_round_{r}\"] = m.group(1).upper() if m else None\n",
        "\n",
        "    print(f\"Completed round {r}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "filename  = (f\"RD_R{ROUNDS}_J_T300_QA600_llama3.3_70b_{timestamp}.json\")\n",
        "\n",
        "with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(tasks, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"All done → {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TE0K7nqW0mzs",
        "outputId": "c4398029-5f03-4350-8ae6-d4514172b6c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All done → RD_R5_J_T300_QA600_llama3.3_70b_20250519_001026.json\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}